
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_example1.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_example1.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_example1.py:


.. _ex1:

Example: Use of MRC with different settings
===========

Example of using MRC with some of the common classification datasets with
different losses and feature mappings settings. We load the different datasets
and use 10-Fold Cross-Validation to generate the partitions for train and test.
We separate 1 partition each time for testing and use the others for training.
On each iteration we calculate the classification error as well as the upper
and lower bounds for the error. We also
calculate the mean training time.

You can check a more elaborated example in :ref:`ex_comp`.

.. GENERATED FROM PYTHON SOURCE LINES 19-122




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    *** Example (MRC with default constraints) *** 


    1. Using 0-1 loss and relu feature mapping 


     ############## 
     mammographic n= 961 , d= 5, cardY= 2
     error= : 0.18000429553264605 +/- 0.027056988666688483
     upper= 0.2137858312132241
     lower= 0.16929532902816125
     avg_train_time= : 2.6179540872573854 secs
     ############## 

     ############## 
     haberman n= 306 , d= 3, cardY= 2
     error= : 0.271505376344086 +/- 0.03809624459797177
     upper= 0.2563259107616672
     lower= 0.21534770421141647
     avg_train_time= : 1.2814617156982422 secs
     ############## 

     ############## 
     indian_liver n= 583 , d= 10, cardY= 2
     error= : 0.28649912331969607 +/- 0.012004226040225736
     upper= 0.2888727901570463
     lower= 0.2717774561287837
     avg_train_time= : 1.9112420082092285 secs
     ############## 

     ############## 
     diabetes n= 768 , d= 8, cardY= 2
     error= : 0.2487012987012987 +/- 0.04247229367633418
     upper= 0.27150553077148476
     lower= 0.21850273268168507
     avg_train_time= : 2.806364369392395 secs
     ############## 

     ############## 
     credit n= 690 , d= 15, cardY= 2
     error= : 0.1492753623188406 +/- 0.042028985507246375
     upper= 0.18043255343556652
     lower= 0.10605660569162896
     avg_train_time= : 2.13475661277771 secs
     ############## 

    2. Using log loss and relu feature mapping 


     ############## 
     mammographic n= 961 , d= 5, cardY= 2
     error= : 0.18206615120274913 +/- 0.031080568307734705
     upper= 0.5198634542352019
     lower= 0.3759055736266058
     avg_train_time= : 8.034570264816285 secs
     ############## 

     ############## 
     haberman n= 306 , d= 3, cardY= 2
     error= : 0.2619354838709677 +/- 0.041750903403655705
     upper= 0.5693554740704263
     lower= 0.45766535846533307
     avg_train_time= : 3.632576847076416 secs
     ############## 

     ############## 
     indian_liver n= 583 , d= 10, cardY= 2
     error= : 0.2779368790181181 +/- 0.0176173967100694
     upper= 0.6002538373763667
     lower= 0.5240726717848576
     avg_train_time= : 5.596090054512024 secs
     ############## 

     ############## 
     diabetes n= 768 , d= 8, cardY= 2
     error= : 0.25133287764866713 +/- 0.03883626035841686
     upper= 0.5840448310710695
     lower= 0.4612889900265441
     avg_train_time= : 9.072916531562806 secs
     ############## 

     ############## 
     credit n= 690 , d= 15, cardY= 2
     error= : 0.1463768115942029 +/- 0.042722907168068626
     upper= 0.4711545255071178
     lower= 0.30196814500331676
     avg_train_time= : 7.632111668586731 secs
     ############## 







|

.. code-block:: default


    import time

    import numpy as np
    from sklearn import preprocessing
    from sklearn.model_selection import StratifiedKFold

    from MRCpy import MRC
    # Import the datasets
    from MRCpy.datasets import *

    # Data sets
    loaders = [load_mammographic, load_haberman, load_indian_liver,
               load_diabetes, load_credit]
    dataName = ["mammographic", "haberman", "indian_liver",
                "diabetes", "credit"]


    def runMRC(phi, loss):

        res_mean = np.zeros(len(dataName))
        res_std = np.zeros(len(dataName))

        # We fix the random seed to that the stratified kfold performed
        # is the same through the different executions
        random_seed = 0

        # Iterate through each of the dataset and fit the MRC classfier.
        for j, load in enumerate(loaders):

            # Loading the dataset
            X, Y = load(return_X_y=True)
            r = len(np.unique(Y))
            n, d = X.shape

            # Print the dataset name
            print(" ############## \n " + dataName[j] + " n= " + str(n) +
                  " , d= " + str(d) + ", cardY= " + str(r))

            clf = MRC(phi=phi, loss=loss, solver='MOSEK',
                      use_cvx=True, max_iters=10000, s=0.3)

            # Generate the partitions of the stratified cross-validation
            cv = StratifiedKFold(n_splits=10, random_state=random_seed,
                                 shuffle=True)

            cvError = list()
            auxTime = 0
            upper = 0
            lower = 0

            # Paired and stratified cross-validation
            for train_index, test_index in cv.split(X, Y):

                X_train, X_test = X[train_index], X[test_index]
                y_train, y_test = Y[train_index], Y[test_index]

                # Normalizing the data
                std_scale = preprocessing.StandardScaler().fit(X_train, y_train)
                X_train = std_scale.transform(X_train)
                X_test = std_scale.transform(X_test)

                # Save start time for computing training time
                startTime = time.time()

                # Train the model and save the upper and lower bounds
                clf.fit(X_train, y_train)
                upper += clf.get_upper_bound()
                lower += clf.get_lower_bound()

                # Save the training time
                auxTime += time.time() - startTime

                # Predict the class for test instances
                y_pred = clf.predict(X_test)

                # Calculate the error made by MRC classificator
                cvError.append(np.average(y_pred != y_test))

            res_mean[j] = np.average(cvError)
            res_std[j] = np.std(cvError)

            # Calculating the mean upper and lower bound and training time
            upper = upper / 10
            lower = lower / 10
            auxTime = auxTime / 10

            print(" error= " + ": " + str(res_mean[j]) + " +/- " +
                  str(res_std[j]))
            print(" upper= " + str(upper) + "\n lower= " + str(lower) +
                  "\n avg_train_time= " + ": " + str(auxTime) + ' secs' +
                  "\n ############## \n")


    if __name__ == '__main__':

        print('*** Example (MRC with default constraints) *** \n\n')

        print('1. Using 0-1 loss and relu feature mapping \n\n')
        runMRC(phi='relu', loss='0-1')

        print('2. Using log loss and relu feature mapping \n\n')
        runMRC(phi='relu', loss='log')


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 7 minutes  27.576 seconds)


.. _sphx_glr_download_auto_examples_plot_example1.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_example1.py <plot_example1.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_example1.ipynb <plot_example1.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
