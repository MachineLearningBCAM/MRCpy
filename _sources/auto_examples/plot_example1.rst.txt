
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_example1.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_example1.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_example1.py:


.. _ex1:

Example: Use of MRC with different settings
===========

Example of using MRC with some of the common classification datasets with
different losses and feature mappings settings. We load the different datasets
and use 10-Fold Cross-Validation to generate the partitions for train and test.
We separate 1 partition each time for testing and use the others for training.
On each iteration we calculate the classification error as well as the upper
and lower bounds for the error. We also
calculate the mean training time.

You can check a more elaborated example in :ref:`ex_comp`.

.. GENERATED FROM PYTHON SOURCE LINES 19-122




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    *** Example (MRC with default constraints) *** 


    1. Using 0-1 loss and relu feature mapping 


     ############## 
     mammographic n= 961 , d= 5, cardY= 2
     error= : 0.17686855670103094 +/- 0.03404213009635484
     upper= 0.21416004633095884
     lower= 0.16557589425238312
     avg_train_time= : 1.993485426902771 secs
     ############## 

     ############## 
     haberman n= 306 , d= 3, cardY= 2
     error= : 0.27172043010752683 +/- 0.042051260087757225
     upper= 0.25535885719913576
     lower= 0.20958190336325444
     avg_train_time= : 0.9947523832321167 secs
     ############## 

     ############## 
     indian_liver n= 583 , d= 10, cardY= 2
     error= : 0.28302162478082993 +/- 0.011388036279538709
     upper= 0.2885205216322503
     lower= 0.27136437450488493
     avg_train_time= : 1.4437336444854736 secs
     ############## 

     ############## 
     diabetes n= 768 , d= 8, cardY= 2
     error= : 0.2526144907723855 +/- 0.04132583902154278
     upper= 0.27035312467294664
     lower= 0.21706150351251874
     avg_train_time= : 2.2149924516677855 secs
     ############## 

     ############## 
     credit n= 690 , d= 15, cardY= 2
     error= : 0.1463768115942029 +/- 0.04122163087921128
     upper= 0.17783198833262276
     lower= 0.1089678559135383
     avg_train_time= : 1.717576503753662 secs
     ############## 

    2. Using log loss and relu feature mapping 


     ############## 
     mammographic n= 961 , d= 5, cardY= 2
     error= : 0.18209836769759452 +/- 0.022903742708359254
     upper= 0.5190333954671233
     lower= 0.3746509295483668
     avg_train_time= : 6.573118686676025 secs
     ############## 

     ############## 
     haberman n= 306 , d= 3, cardY= 2
     error= : 0.26526881720430107 +/- 0.040995954499815065
     upper= 0.5677953625480997
     lower= 0.45509876858067766
     avg_train_time= : 3.720004606246948 secs
     ############## 

     ############## 
     indian_liver n= 583 , d= 10, cardY= 2
     error= : 0.28468731735827 +/- 0.028469786996262708
     upper= 0.6001270538248882
     lower= 0.5357824452851391
     avg_train_time= : 5.274538779258728 secs
     ############## 

     ############## 
     diabetes n= 768 , d= 8, cardY= 2
     error= : 0.24084073820915924 +/- 0.04301716498399907
     upper= 0.5832217359007436
     lower= 0.46180613901161854
     avg_train_time= : 7.90848126411438 secs
     ############## 

     ############## 
     credit n= 690 , d= 15, cardY= 2
     error= : 0.14492753623188406 +/- 0.04150093061819233
     upper= 0.4672274445461337
     lower= 0.29721227999709665
     avg_train_time= : 6.290561175346374 secs
     ############## 







|

.. code-block:: default


    import time

    import numpy as np
    from sklearn import preprocessing
    from sklearn.model_selection import StratifiedKFold

    from MRCpy import MRC
    # Import the datasets
    from MRCpy.datasets import *

    # Data sets
    loaders = [load_mammographic, load_haberman, load_indian_liver,
               load_diabetes, load_credit]
    dataName = ["mammographic", "haberman", "indian_liver",
                "diabetes", "credit"]


    def runMRC(phi, loss):

        res_mean = np.zeros(len(dataName))
        res_std = np.zeros(len(dataName))

        # We fix the random seed to that the stratified kfold performed
        # is the same through the different executions
        random_seed = 0

        # Iterate through each of the dataset and fit the MRC classfier.
        for j, load in enumerate(loaders):

            # Loading the dataset
            X, Y = load()
            r = len(np.unique(Y))
            n, d = X.shape

            # Print the dataset name
            print(" ############## \n " + dataName[j] + " n= " + str(n) +
                  " , d= " + str(d) + ", cardY= " + str(r))

            clf = MRC(phi=phi, loss=loss, solver='MOSEK',
                      use_cvx=True, max_iters=10000, s=0.3)

            # Generate the partitions of the stratified cross-validation
            cv = StratifiedKFold(n_splits=10, random_state=random_seed,
                                 shuffle=True)

            cvError = list()
            auxTime = 0
            upper = 0
            lower = 0

            # Paired and stratified cross-validation
            for train_index, test_index in cv.split(X, Y):

                X_train, X_test = X[train_index], X[test_index]
                y_train, y_test = Y[train_index], Y[test_index]

                # Normalizing the data
                std_scale = preprocessing.StandardScaler().fit(X_train, y_train)
                X_train = std_scale.transform(X_train)
                X_test = std_scale.transform(X_test)

                # Save start time for computing training time
                startTime = time.time()

                # Train the model and save the upper and lower bounds
                clf.fit(X_train, y_train)
                upper += clf.get_upper_bound()
                lower += clf.get_lower_bound()

                # Save the training time
                auxTime += time.time() - startTime

                # Predict the class for test instances
                y_pred = clf.predict(X_test)

                # Calculate the error made by MRC classificator
                cvError.append(np.average(y_pred != y_test))

            res_mean[j] = np.average(cvError)
            res_std[j] = np.std(cvError)

            # Calculating the mean upper and lower bound and training time
            upper = upper / 10
            lower = lower / 10
            auxTime = auxTime / 10

            print(" error= " + ": " + str(res_mean[j]) + " +/- " +
                  str(res_std[j]))
            print(" upper= " + str(upper) + "\n lower= " + str(lower) +
                  "\n avg_train_time= " + ": " + str(auxTime) + ' secs' +
                  "\n ############## \n")


    if __name__ == '__main__':

        print('*** Example (MRC with default constraints) *** \n\n')

        print('1. Using 0-1 loss and relu feature mapping \n\n')
        runMRC(phi='relu', loss='0-1')

        print('2. Using log loss and relu feature mapping \n\n')
        runMRC(phi='relu', loss='log')


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 6 minutes  21.606 seconds)


.. _sphx_glr_download_auto_examples_plot_example1.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_example1.py <plot_example1.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_example1.ipynb <plot_example1.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
