{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Example: Comparison to other methods\nWe are training and testing both MRC and CMRC methods with\na variety of different settings and comparing their performance both\nerror-wise and time-wise to other usual classification methods.\n\nWe will see that the performance of the MRC methods with the appropiate\nsettings is similar to the one of other methods like SVC (SVM Classification)\nor MLPClassifier (neural network).\nFurthermore, with non-determinitic approach and loss 0-1,\nMRC method provides a theoretical upper and lower bound for the error\nthat can be an useful non-biased indicator of the performance of the\nalgorithm on a given dataset.\nIt also can be used to perform hyperparameter tuning in a much faster way than\ncross-validation, you can check an example about that `here<grid>`.\n\nWe show the numerical results in three tables; the two firsts ones for all\nthe MRC and CMRC variants and the next one for all the comparison methods\nin the deterministic and non-deterministic case respectively.\nIn these firsts tables the columns named 'upper' and 'lower' show the\nupper and lower bound provided by the MRC method.\nNote that in the case where loss = `0-1` these are upper and\nlower bounds of the classification error while, in the case of `loss=log`\nthese bounds correspond to the log-likelihood.\n\nNote that we set the parameter use_cvx=False. In the case of MRC classifiers\nthis means that we will use nesterov subgradient optimized approach to\nperform the optimization. In the case of CMRC classifiers it will use the fast\nStochastic Gradient Descent (SGD) approach for linear and random fourier\nfeature mappings and nesterov subgradient approach for the rest of feature\nmappings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Import needed modules\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\n\nfrom MRCpy import CMRC, MRC\nfrom MRCpy.datasets import load_credit, load_haberman\n\n\nKFOLDS = 5\nkf = KFold(n_splits=KFOLDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MRC and CMRC methods\nWe are training and testing both MRC and CMRC methods with\na variety of different settings; using 0-1 loss and logarithmic loss, using\nall the default feature mappings available (Linear, Random Fourier, ReLU,\nThreshold) and using both the non-deterministic and deterministic\napproach which uses or not,\nrespectively probability estimates in the prediction stage.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def runMRC(X, Y):\n    df_mrc = pd.DataFrame(np.zeros((8, 4)),\n                          columns=['MRC', 'MRC time', 'CMRC', 'CMRC time'],\n                          index=['loss 0-1, phi linear',\n                                 'loss 0-1, phi fourier',\n                                 'loss 0-1, phi relu',\n                                 'loss 0-1, phi threshold',\n                                 'loss log, phi linear',\n                                 'loss log, phi fourier',\n                                 'loss log, phi relu',\n                                 'loss log, phi threshold'])\n\n    df_mrc_nd = pd.DataFrame(np.zeros((4, 4)),\n                             columns=['MRC', 'MRC time', 'upper', 'lower'],\n                             index=['loss 0-1, phi linear',\n                                    'loss 0-1, phi fourier',\n                                    'loss 0-1, phi relu',\n                                    'loss 0-1, phi threshold'])\n\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        Y_train, Y_test = Y[train_index], Y[test_index]\n        std_scale = preprocessing.StandardScaler().fit(X_train, Y_train)\n        X_train = std_scale.transform(X_train)\n        X_test = std_scale.transform(X_test)\n\n        for loss in ['0-1', 'log']:\n            for phi in ['linear', 'fourier', 'relu', 'threshold']:\n                row_name = 'loss ' + loss + ', phi ' + phi\n\n                # Deterministic case\n                startTime = time.time()\n                clf = MRC(loss=loss, phi=phi, random_state=0, sigma='scale',\n                          deterministic=True, use_cvx=False\n                          ).fit(X_train, Y_train)\n                Y_pred = clf.predict(X_test)\n                error = np.average(Y_pred != Y_test)\n                totalTime = time.time() - startTime\n\n                df_mrc['MRC time'][row_name] += totalTime\n                df_mrc['MRC'][row_name] += error\n\n                startTime = time.time()\n                clf = CMRC(loss=loss, phi=phi, random_state=0, sigma='scale',\n                           deterministic=True, use_cvx=False,\n                           ).fit(X_train, Y_train)\n                Y_pred = clf.predict(X_test)\n                error = np.average(Y_pred != Y_test)\n                totalTime = time.time() - startTime\n\n                df_mrc['CMRC time'][row_name] += totalTime\n                df_mrc['CMRC'][row_name] += error\n\n                if loss == '0-1':\n                    # Non-deterministic case (with upper-lower bounds)\n                    startTime = time.time()\n                    clf = MRC(loss=loss, phi=phi, random_state=0,\n                              sigma='scale',\n                              deterministic=False, use_cvx=False,\n                              ).fit(X_train, Y_train)\n                    Y_pred = clf.predict(X_test)\n                    error = np.average(Y_pred != Y_test)\n                    totalTime = time.time() - startTime\n\n                    df_mrc_nd['MRC time'][row_name] += totalTime\n                    df_mrc_nd['MRC'][row_name] += error\n                    df_mrc_nd['upper'][row_name] += clf.get_upper_bound()\n                    df_mrc_nd['lower'][row_name] += clf.get_lower_bound()\n\n    df_mrc = df_mrc.divide(KFOLDS)\n    df_mrc_nd = df_mrc_nd.divide(KFOLDS)\n    return df_mrc, df_mrc_nd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the non deterministic linear case is expected to perform poorly\nfor datasets with small initial dimensions\nlike the ones in the example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Credit dataset\nX, Y = load_credit()\ndf_mrc_credit, df_mrc_nd_credit = runMRC(X, Y)\ndf_mrc_credit.style.set_caption('Credit Dataset: Deterministic \\\n                                MRC and CMRC error and runtime')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_mrc_nd_credit.style.set_caption('Credit Dataset: Non-Deterministic \\\n                                   MRC error and runtime\\nwith Upper and\\\n                                       Lower bounds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Haberman Dataset\nX, Y = load_haberman()\ndf_mrc_haberman, df_mrc_nd_haberman = runMRC(X, Y)\ndf_mrc_haberman.style.set_caption('Haberman Dataset: Deterministic \\\n                                  MRC and CMRC error and runtime')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_mrc_nd_haberman.style.set_caption('Haberman Dataset: Non-Deterministic MRC \\\n                                     error and runtime\\nwith Upper and \\\n                                         Lower bounds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SVM, Neural Networks: MLP Classifier, Random Forest Classifier\nNow, let's try other usual supervised classification algorithms and compare\nthe results.\nFor comparison purposes. We try the same experiment using the Support Vector\nMachine method using C-Support Vector Classification implemented in the\n`SVC<https://scikit-learn.org/stable/modules/\ngenerated/sklearn.svm.SVC.html>`\nfunction, the Neural Network\nmethod `Multi-layer Perceptron classifier<https://scikit-learn.org/\nstable/modules/generated/sklearn.neural_network.MLPClassifier.html>`\nand a `Random Forest\nClassifier<https://scikit-learn.org/stable/modules/generated/\nsklearn.ensemble.RandomForestClassifier.html>`.\nAll of them from the library `scikit-learn`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def runComparisonMethods(X, Y):\n    df = pd.DataFrame(columns=['Method', 'Error', 'Time'])\n\n    error_svm = 0\n    totalTime_svm = 0\n    error_mlp = 0\n    totalTime_mlp = 0\n    error_rf = 0\n    totalTime_rf = 0\n\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        Y_train, Y_test = Y[train_index], Y[test_index]\n        std_scale = preprocessing.StandardScaler().fit(X_train, Y_train)\n        X_train = std_scale.transform(X_train)\n        X_test = std_scale.transform(X_test)\n\n        startTime = time.time()\n        clf = SVC(random_state=0).fit(X_train, Y_train)\n        Y_pred = clf.predict(X_test)\n        error_svm += np.average(Y_pred != Y_test)\n        totalTime_svm += time.time() - startTime\n\n        startTime = time.time()\n        clf = MLPClassifier(random_state=0).fit(X_train, Y_train)\n        Y_pred = clf.predict(X_test)\n        error_mlp += np.average(Y_pred != Y_test)\n        totalTime_mlp += time.time() - startTime\n\n        startTime = time.time()\n        clf = clf = RandomForestClassifier(\n            max_depth=2, random_state=0).fit(X_train, Y_train)\n        Y_pred = clf.predict(X_test)\n        error_rf += np.average(Y_pred != Y_test)\n        totalTime_rf += time.time() - startTime\n\n    error_svm /= KFOLDS\n    totalTime_svm /= KFOLDS\n    error_mlp /= KFOLDS\n    totalTime_mlp /= KFOLDS\n    error_rf /= KFOLDS\n    totalTime_rf /= KFOLDS\n\n    df = df.append({'Method': 'SVM', 'Error': error_svm,\n                    'Time': totalTime_svm}, ignore_index=True)\n    df = df.append({'Method': 'NN-MLP', 'Error': error_mlp,\n                    'Time': totalTime_mlp}, ignore_index=True)\n    df = df.append({'Method': 'Random Forest', 'Error': error_rf,\n                    'Time': totalTime_rf}, ignore_index=True)\n    df = df.set_index('Method')\n    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Credit Dataset\nX, Y = load_credit()\ndf_credit = runComparisonMethods(X, Y)\ndf_credit.style.set_caption('Credit Dataset: Different \\\n                            methods error and runtime')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Haberman Dataset\nX, Y = load_haberman()\ndf_haberman = runComparisonMethods(X, Y)\ndf_haberman.style.set_caption('Haberman Dataset: Different \\\n                              methods error and runtime')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison of MRCs to other methods\nIn the deterministic case we can see that the performance of MRC and CMRC\nmethods in the\nappropiate settings is similar to usual methods such as SVM and\nNeural Networks implemented by the MLPClassifier. Best performances for MRC\nmethod are usually reached using loss = `0-1` and phi = `fourier` or\nphi = `relu`. Even though these\nsettings make the execution time of MRC a little bit higher than others it\nis still  similar to the time it would take to use the MLPClassifier.\n\nNow we are plotting some figures for the **deterministic** case.\n\nNote that\nthe options of MRC with loss = `0-1` use an optimized version of Nesterov\noptimization algorithm, improving the runtime of these options.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Graph plotting\ndef major_formatter(x, pos):\n    label = '' if x < 0 else '%0.2f' % x\n    return label\n\n\ndef major_formatter1(x, pos):\n    label = '' if x < 0 or x > 0.16 else '%0.3f' % x\n    return label\n\n\ndef major_formatter2(x, pos):\n    label = '' if x < 0 else '%0.2g' % x\n    return label\n\n\nfig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1])\nlabels = ['CMRC\\n0-1\\nlinear',\n          'MRC\\n0-1\\nrelu',\n          'MRC\\n0-1\\nthreshold',\n          'MRC\\nlog\\nthreshold',\n          'SVM', 'NN-MLP',\n          'Random\\nforest']\n\nerrors = [df_mrc_credit['CMRC']['loss 0-1, phi linear'],\n          df_mrc_credit['MRC']['loss 0-1, phi relu'],\n          df_mrc_credit['MRC']['loss 0-1, phi threshold'],\n          df_mrc_credit['MRC']['loss log, phi threshold'],\n          df_credit['Error']['SVM'],\n          df_credit['Error']['NN-MLP'],\n          df_credit['Error']['Random Forest']]\nax.bar([''] + labels, [0] + errors, color='lightskyblue')\nplt.title('Credit Dataset Errors')\nax.tick_params(axis=\"y\", direction=\"in\", pad=-35)\nax.tick_params(axis=\"x\", direction=\"out\", pad=-40)\nax.yaxis.set_major_formatter(major_formatter1)\nmargin = 0.05 * max(errors)\nax.set_ylim([-margin * 3.5, max(errors) + margin])\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above: MRCs errors for different parameter settings\ncompared to other techniques for the dataset Credit. The ordinate\naxis represents the error (proportion of incorrectly predicted labels).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1])\n\nlabels = ['MRC\\n0-1\\nrelu',\n          'MRC\\n0-1\\nthreshold',\n          'SVM', 'NN-MLP',\n          'Random\\nforest']\n\ntimes = [df_mrc_credit['MRC time']['loss 0-1, phi relu'],\n         df_mrc_credit['MRC time']['loss 0-1, phi threshold'],\n         df_credit['Time']['SVM'],\n         df_credit['Time']['NN-MLP'],\n         df_credit['Time']['Random Forest']]\nax.bar([''] + labels, [0] + times, color='lightskyblue')\nplt.title('Credit Dataset Runtime')\nax.tick_params(axis=\"y\", direction=\"in\", pad=-30)\nax.tick_params(axis=\"x\", direction=\"out\", pad=-40)\nax.yaxis.set_major_formatter(major_formatter2)\nmargin = 0.05 * max(times)\nax.set_ylim([-margin * 3.5, max(times) + margin])\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above: MRCs runtime for different parameter settings\ncompared to other techniques for the dataset Credit. The ordinate\nrepresents the runtime measured in seconds.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1])\nlabels = ['MRC\\n0-1\\nfourier',\n          'CMRC\\n0-1\\nfourier',\n          'SVM',\n          'NN-MLP',\n          'Random\\nforest']\n\nerrors = [df_mrc_haberman['MRC']['loss 0-1, phi fourier'],\n          df_mrc_haberman['CMRC']['loss 0-1, phi fourier'],\n          df_haberman['Error']['SVM'],\n          df_haberman['Error']['NN-MLP'],\n          df_haberman['Error']['Random Forest']]\nax.bar([''] + labels, [0] + errors, color='lightskyblue')\nplt.title('Haberman Dataset Errors')\nax.tick_params(axis=\"y\", direction=\"in\", pad=-30)\nax.tick_params(axis=\"x\", direction=\"out\", pad=-40)\nax.yaxis.set_major_formatter(major_formatter)\nmargin = 0.05 * max(errors)\nax.set_ylim([-margin * 3.5, max(errors) + margin])\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above: MRCs errors for different parameter settings\ncompared to other techniques for the dataset Haberman. The ordinate\naxis represents the error (proportion of incorrectly predicted labels).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\nax = fig.add_axes([0, 0, 1, 1])\n\nlabels = ['MRC\\n0-1\\nfourier',\n          'MRC\\n0-1\\nrelu',\n          'SVM', 'NN-MLP',\n          'Random\\nforest']\n\ntimes = [df_mrc_haberman['MRC time']['loss 0-1, phi fourier'],\n         df_mrc_haberman['MRC time']['loss 0-1, phi relu'],\n         df_haberman['Time']['SVM'],\n         df_haberman['Time']['NN-MLP'],\n         df_haberman['Time']['Random Forest']]\nax.bar([''] + labels, [0] + times, color='lightskyblue')\nplt.title('Haberman Dataset Runtime')\nax.tick_params(axis=\"y\", direction=\"in\", pad=-30)\nax.tick_params(axis=\"x\", direction=\"out\", pad=-40)\nax.yaxis.set_major_formatter(major_formatter2)\nmargin = 0.05 * max(times)\nax.set_ylim([-margin * 3.5, max(times) + margin])\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above: MRCs runtime for different parameter settings\ncompared to other techniques for the dataset Haberman. The ordinate\nrepresents the runtime measured in seconds.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upper and Lower bounds provided by MRCs\nFurthermore, when using a non-deterministic approach and `loss = 0-1`, the\nMRC method provides us with Upper and Lower theoretical bounds for the\nerror which can be of great use to make sure you are not overfitting your\nmodel or for hyperparameter tuning. You can check our\n`example on parameter tuning<grid>`.\nIn the logistic case these Upper and Lower values are the theoretical bounds\nfor the log-likelihood.\n\nThe only difference between the deterministic and  non-deterministic approach\nis in the prediction stage so, as we can see, the runtime of both versions\nis pretty similar.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}