{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n\n# Example: Predicting COVID-19 patients outcome using MRCs\n\nIn this example we will use `MRCpy.MRC` and `MRCpy.CMRC` to predict the outcome\nof a COVID-19 positive patient at the moment of hospital triage. This example\nuses a dataset that comprises different demographic variables and biomarkers of\nthe patients and a binary outcome :attr:`Status` where :attr:`Status = 0`\ndefine the group of survivors and :attr:`Status = 1` determines a decease.\n\nThe data is provided by the `Covid Data Saves Lives\n<https://www.hmhospitales.com/coronavirus/covid-data-save-lives/>`_ initiative\ncarried out by HM Hospitales with information of the first wave of the COVID\noutbreak in Spanish hospitals. The data is available upon request through HM\nHospitales\n`here <https://www.hmhospitales.com/coronavirus/covid-data-save-lives/>`_ .\n\n.. seealso::    For more information about the dataset and the creation of a\n                risk indicator using Logistic regression refer to:\n\n                [1] Ruben Arma\u00f1anzas et al. \u201cDerivation of a Cost-Sensitive\n                COVID-19 Mortality Risk Indicator Using a Multistart Framework\"\n                , in *2021 IEEE International Conference on Bioinformatics and\n                Biomedicine (BIBM)*, 2021, pp. 2179\u20132186.\n\n\n\nFirst we will see how to deal with class imbalance when training a model using\nsyntethic minority over-sampling (SMOTE) techniques. Furthermore, we will\ncomparetwo MRC with two state of the art machine learning models probability\nestimation . The selected models are :mod:`CMRC(phi = 'threshold' ,\nloss = 'log')` & :mod:`MRC(phi = 'fourier' , loss = 'log')` for  the group of\nMRCs and Logistic Regression (LR) & C-Support Vector Classifier(SVC) with the\nimplementation from `Scikit-Learn <https://scikit-learn.org/stable/#>`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Import needed modules\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTENC\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import (\n    classification_report,\n    confusion_matrix,\n    ConfusionMatrixDisplay,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\nfrom MRCpy import CMRC, MRC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## COVID dataset Loader:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def load_covid(norm=False, array=True):\n    data_consensus = pd.read_csv(\"data/data_consensus.csv\", sep=\";\")\n    # rename variables\n    variable_dict = {\n        \"CD0000AGE\": \"Age\",\n        \"CORE\": \"PATIENT_ID\",\n        \"CT000000U\": \"Urea\",\n        \"CT00000BT\": \"Bilirubin\",\n        \"CT00000NA\": \"Sodium\",\n        \"CT00000TP\": \"Proth_time\",\n        \"CT0000COM\": \"Com\",\n        \"CT0000LDH\": \"LDH\",\n        \"CT0000NEU\": \"Neutrophils\",\n        \"CT0000PCR\": \"Pro_C_Rea\",\n        \"CT0000VCM\": \"Med_corp_vol\",\n        \"CT000APTT\": \"Ceph_time\",\n        \"CT000CHCM\": \"Mean_corp_Hgb\",\n        \"CT000EOSP\": \"Eosinophils%\",\n        \"CT000LEUC\": \"Leukocytes\",\n        \"CT000LINP\": \"Lymphocytes%\",\n        \"CT000NEUP\": \"Neutrophils%\",\n        \"CT000PLAQ\": \"Platelet_count\",\n        \"CTHSDXXRATE\": \"Rate\",\n        \"CTHSDXXSAT\": \"Sat\",\n        \"ED0DISWHY\": \"Status\",\n        \"F_INGRESO/ADMISSION_D_ING/INPAT\": \"Fecha_admision\",\n        \"SEXO/SEX\": \"Sexo\",\n    }\n    data_consensus = data_consensus.rename(columns=variable_dict)\n    if norm:  # if we want the data standardised\n        x_consensus = data_consensus[\n            data_consensus.columns.difference([\"Status\", \"PATIENT_ID\"])\n        ][:]\n        std_scale = preprocessing.StandardScaler().fit(x_consensus)\n        x_consensus_std = std_scale.transform(x_consensus)\n        dataframex_consensus = pd.DataFrame(\n            x_consensus_std, columns=x_consensus.columns\n        )\n        data_consensus.reset_index(drop=True, inplace=True)\n        data_consensus = pd.concat(\n            [dataframex_consensus, data_consensus[[\"Status\"]]], axis=1\n        )\n    data_consensus = data_consensus[data_consensus.columns.difference(\n        [\"PATIENT_ID\"])]\n    X = data_consensus[data_consensus.columns.difference(\n        [\"Status\", \"PATIENT_ID\"])]\n    y = data_consensus[\"Status\"]\n    if array:\n        X = X.to_numpy()\n        y = y.to_numpy()\n    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Addressing dataset imbalance with SMOTE\nThe COVID dataset has a significant problem of class imbalance where the\npositive outcome has a prevalence of 85% (1522) whilst the negative outcome\nhas only 276. In this example oversampling will be used to add syintetic\nrecords to get an almost balanced dataset. :mod:`SMOTE` (Synthetic minority\nover sampling) is a package that implements such oversampling.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = load_covid(array=False)\ndescribed = X.describe(percentiles=[0.5]).round(\n    2).transpose()[[\"count\", \"mean\", \"std\"]]\npd.DataFrame(y.value_counts().rename({0.0: \"Survive\", 1.0: \"Decease\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we create a set of cases syntehtically using 5 nearest neighbors until\nthe class imbalance is almost removed. For more information about\n:mod:`SMOTE` refer to it's `documentation\n<https://imbalanced-learn.org/stable/>`_ .\nWe will use the method `SMOTE-NC` for numerical and categorical variables.\n\n.. seealso::    For more information about the SMOTE package refer to:\n\n               [2] Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer,\n               W. P. (2002). SMOTE: synthetic minority over-sampling \n               technique. Journal of artificial intelligence research,\n               16, 321-357.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We fit the data to the oversampler\nsmotefit = SMOTENC(sampling_strategy=0.75, categorical_features=[3])\nX_resampled, y_resampled = smotefit.fit_resample(X, y)\ndescribed_resample = (\n    X_resampled.describe(percentiles=[0.5])\n    .round(2)\n    .transpose()[[\"count\", \"mean\", \"std\"]]\n)\ndescribed_resample = described_resample.add_suffix(\"_SMT\")\npd.concat([described, described_resample], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see how the distribution of the real data and the resampled data is\ndifferent. However the distribution between classes is kept similar due to\nthe creation of the synthetic cases through 5 nearest neighbors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(y_resampled.value_counts().rename(\n    {0.0: \"Survive\", 1.0: \"Decease\"}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Probability estimation\nIn this section we will estimate the conditional probabilities and analyse\nthe distribution of the probabilities depending on the real outcome . The\nprobability estimation is better when using :mod:`loss = log`. We use\n:mod:`CMRC(phi = 'threshold', loss = 'log')` and\n:mod:`MRC(phi = 'fourier' , loss = 'log'`. We will then compare these MRCs\nwith SVC and LR with default parameters.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load classification function:\nThese function classify each of the cases in their correspondent\nconfusion matrix's category. It also allows to set the desired cut-off\nfor the predictions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def defDataFrame(model, x_test, y_test, threshold=0.5):\n    \"\"\"\n    Takes x,y test and train and a fitted model and\n    computes the probabilities to then classify in TP,TN , FP , FN.\n    \"\"\"\n    if \"predict_proba\" in dir(model):\n        probabilities = model.predict_proba(x_test)[:, 1]\n        predictions = [1 if i > threshold else 0 for i in probabilities]\n        df = pd.DataFrame(\n            {\n                \"Real\": y_test.tolist(),\n                \"Prediction\": predictions,\n                \"Probabilities\": probabilities.tolist(),\n            }\n        )\n    else:\n        df = pd.DataFrame(\n            {\"Real\": y_test.tolist(), \"Prediction\": model.predict(x_test)}\n        )\n    conditions = [\n        (df[\"Real\"] == 1) & (df[\"Prediction\"] == 1),\n        (df[\"Real\"] == 1) & (df[\"Prediction\"] == 0),\n        (df[\"Real\"] == 0) & (df[\"Prediction\"] == 0),\n        (df[\"Real\"] == 0) & (df[\"Prediction\"] == 1),\n    ]\n    choices = [\n        \"True Positive\",\n        \"False Negative\",\n        \"True Negative\",\n        \"False Positive\",\n    ]\n    df[\"Category\"] = np.select(conditions, choices, default=\"No\")\n    df.sort_index(inplace=True)\n    df.sort_values(by=\"Category\", ascending=False, inplace=True)\n    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train models:\nWe will train the models with 80% of the data and then test with the other\n20% selected randomly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n    X_resampled, y_resampled, test_size=0.2, random_state=1\n)\n\nclf_MRC = MRC(phi=\"fourier\", use_cvx=True, loss=\"log\").fit(X_train, y_train)\ndf_MRC = defDataFrame(model=clf_MRC, x_test=X_test, y_test=y_test)\nMRC_values = pd.DataFrame(df_MRC.Category.value_counts()).rename(\n    columns={\"Category\": type(clf_MRC).__name__}\n)\nMRC_values[\"Freq_MRC\"] = MRC_values[\"MRC\"] / sum(MRC_values[\"MRC\"]) * 100\n\nclf_CMRC = CMRC(phi=\"threshold\", use_cvx=True,\n                loss=\"log\").fit(X_train, y_train)\ndf_CMRC = defDataFrame(model=clf_CMRC, x_test=X_test, y_test=y_test)\nCMRC_values = pd.DataFrame(df_CMRC.Category.value_counts()).rename(\n    columns={\"Category\": type(clf_CMRC).__name__}\n)\nCMRC_values[\"Freq_CMRC\"] = CMRC_values[\"CMRC\"] / sum(CMRC_values[\"CMRC\"]) * 100\n\nclf_SVC = SVC(probability=True).fit(X_train, y_train)\ndf_SVC = defDataFrame(model=clf_SVC, x_test=X_test, y_test=y_test)\nSVC_values = pd.DataFrame(df_SVC.Category.value_counts()).rename(\n    columns={\"Category\": type(clf_SVC).__name__}\n)\nSVC_values[\"Freq_SVC\"] = SVC_values[\"SVC\"] / sum(SVC_values[\"SVC\"]) * 100\n\nclf_LR = LogisticRegression().fit(X_train, y_train)\ndf_LR = defDataFrame(model=clf_LR, x_test=X_test, y_test=y_test)\nLR_values = pd.DataFrame(df_LR.Category.value_counts()).rename(\n    columns={\"Category\": type(clf_LR).__name__}\n)\nLR_values[\"Freq_LR\"] = (\n    LR_values[\"LogisticRegression\"] /\n    sum(LR_values[\"LogisticRegression\"]) * 100\n)\n\n\npd.concat([MRC_values, CMRC_values, SVC_values,\n           LR_values], axis=1).style.set_caption(\n    \"Classification results by model\"\n).format(precision=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison of models:\nWe will compare now the histograms of the conditional probability for the\ntwo posible outcomes. Overlapping in the histograms means that the\nclassification is erroneous. Condisering a cutoff of 0.5 pink cases below\nthis point are false negatives (FN) and blue cases above the threhsold false\npositives (FP). It is important to consider that in this classification\nproblem the missclassification of a patient with fatal outcome (FN) is\nconsidered a much more serious error.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scatterPlot(df, ax):\n    \"\"\"\n    Takes DF created with defDataFrame and creates a boxplot of\n    different classification by mortal probability.\n    \"\"\"\n    sns.swarmplot(\n        ax=ax,\n        y=\"Category\",\n        x=\"Probabilities\",\n        data=df,\n        size=4,\n        palette=sns.color_palette(\"tab10\"),\n        linewidth=0,\n        dodge=False,\n        alpha=0.6,\n        order=[\"True Negative\", \"False Negative\",\n               \"True Positive\", \"False Positive\", ],\n    )\n    sns.boxplot(\n        ax=ax,\n        x=\"Probabilities\",\n        y=\"Category\",\n        color=\"White\",\n        data=df,\n        order=[\"True Negative\", \"False Negative\",\n               \"True Positive\", \"False Positive\", ],\n        saturation=15,\n    )\n    ax.set_xlabel(\"Probability of mortality\")\n    ax.set_ylabel(\"\")\n\n\ndef plotHisto(df, ax, threshold=0.5, normalize=True):\n    \"\"\"\n    Takes DF created with defDataFrame and plots histograms based on the\n    probability of mortality by real Status at a selected @threshold.\n    \"\"\"\n    if normalize:\n        norm_params = {\"stat\": \"density\", \"common_norm\": False}\n    else:\n        norm_params = {}\n    sns.histplot(\n        ax=ax,\n        data=df[df[\"Real\"] == 1],\n        x=\"Probabilities\",\n        color=\"deeppink\",\n        label=\"Deceased\",\n        bins=15,\n        binrange=[0, 1],\n        alpha=0.6,\n        element=\"step\",\n        **norm_params\n    )\n    sns.histplot(\n        ax=ax,\n        data=df[df[\"Real\"] == 0],\n        x=\"Probabilities\",\n        color=\"dodgerblue\",\n        label=\"Survived\",\n        bins=15,\n        binrange=[0, 1],\n        alpha=0.4,\n        element=\"step\",\n        **norm_params\n    )\n    ax.axvline(threshold, 0, 1, linestyle=(\n        0, (1, 10)), linewidth=0.7, color=\"black\")\n\n\n# visualize results\nfig, ax = plt.subplots(\n    nrows=2,\n    ncols=2,\n    sharex=\"all\",\n    sharey=\"all\",\n    gridspec_kw={\"wspace\": 0.1, \"hspace\": 0.35},\n)\nplotHisto(df_CMRC, ax=ax[0, 0], normalize=False)\nax[0, 0].set_title(\"CMRC\")\nplotHisto(df_MRC, ax=ax[1, 0], normalize=False)\nax[1, 0].set_title(\"MRC\")\nplotHisto(df_LR, ax=ax[0, 1], normalize=False)\nax[0, 1].set_title(\"LR\")\nax[0, 1].legend()\nplotHisto(df_SVC, ax=ax[1, 1], normalize=False)\nax[1, 1].set_title(\"SVC\")\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see a clear different behaviour with the CMRC and MRC. MRC tends to\nestimate conditional probabilities in a more conservative way, rangin from\n0.25 to 0.75. This estimation is very sensible to cut-off changes. The CMRC\nmodel shows a distribution where most of the cases are grouped around 0 and 1\nfor survive and decease respectively. This results are similar to the\nLogistic Regression's but with less overlapping. SVC is the model with the\nworst performance of all having a lot of patients that survived with high\ndecease probabilities.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cm_cmrc = confusion_matrix(y_test, clf_CMRC.predict(X_test))  # CMRC\ncm_mrc = confusion_matrix(y_test, clf_MRC.predict(X_test))  # MRC\ncm_lr = confusion_matrix(y_test, clf_LR.predict(X_test))  # Logistic Regression\ncm_svc = confusion_matrix(y_test, clf_SVC.predict(\n    X_test))  # C-Support Vector Machine\n\nfig, ax = plt.subplots(\n    nrows=2,\n    ncols=2,\n    sharex=\"all\",\n    sharey=\"all\",\n    gridspec_kw={\"wspace\": 0, \"hspace\": 0.35},\n)\nConfusionMatrixDisplay(cm_cmrc, display_labels=[\"Survive\", \"Decease\"]).plot(\n    colorbar=False, ax=ax[0, 0]\n)\nax[0, 0].set_title(\"CMRC\")\nConfusionMatrixDisplay(cm_mrc, display_labels=[\"Survive\", \"Decease\"]).plot(\n    colorbar=False, ax=ax[1, 0]\n)\nax[1, 0].set_title(\"MRC\")\nConfusionMatrixDisplay(cm_lr, display_labels=[\"Survive\", \"Decease\"]).plot(\n    colorbar=False, ax=ax[0, 1]\n)\nax[0, 1].set_title(\"LR\")\nConfusionMatrixDisplay(cm_svc, display_labels=[\"Survive\", \"Decease\"]).plot(\n    colorbar=False, ax=ax[1, 1]\n)\nax[1, 1].set_title(\"SVC\")\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(\n    classification_report(\n        y_test,\n        clf_CMRC.predict(X_test),\n        target_names=[\"Survive\", \"Decease\"],\n        output_dict=True,\n    )\n).style.set_caption(\"Classification report CMRC\").format(precision=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(\n    classification_report(\n        y_test,\n        clf_MRC.predict(X_test),\n        target_names=[\"Survive\", \"Decease\"],\n        output_dict=True,\n    )\n).style.set_caption(\"Classification report MRC\").format(precision=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(\n    classification_report(\n        y_test,\n        clf_LR.predict(X_test),\n        target_names=[\"Survive\", \"Decease\"],\n        output_dict=True,\n    )\n).style.set_caption(\"Classification report LR\").format(precision=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(\n    classification_report(\n        y_test,\n        clf_SVC.predict(X_test),\n        target_names=[\"Survive\", \"Decease\"],\n        output_dict=True,\n    )\n).style.set_caption(\"Classification report SVC\").format(precision=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see in the classification reports and the confusion matrices the\noutperformance of CMRC.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Settind the cut-off point for binary classification:\nIn this section we will use beeswarm-boxplot to select the cut-off point\nto optimise the tradeoff between false positives and false negatives. The\nbeeswarm-boxplot is a great tool to determine the performance of the model\nin each of the cases of the confusion matrix. On an ideal scenario the errors\nare located near the cut-off point and the true guesses are located near the\n0 and 1 values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(\n    nrows=2,\n    ncols=2,\n    figsize=(10, 12),\n    sharex=\"all\",\n    sharey=\"all\",\n    gridspec_kw={\"wspace\": 0.1, \"hspace\": 0.20},\n)\nscatterPlot(df_CMRC, ax[0, 0])\nax[0, 0].set_title(\"CMRC\")\nscatterPlot(df_MRC, ax[1, 0])\nax[1, 0].set_title(\"MRC\")\nscatterPlot(df_LR, ax[0, 1])\nax[0, 1].set_title(\"LR\")\nscatterPlot(df_SVC, ax[1, 1])\nax[1, 1].set_title(\"SVC\")\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see in the CMRC that the correct cases have a very good\nconditional probability estimation with around 75% of the cases very close to\nthe extreme values. The most problematic cases are those with a low mortality\nprobability estimation that had a fatal outcome (FN). In the CMRC\nmodel adjusting the threshold to 0.35 reduces the false negatives by 25%\nadding just some cases to the FP. In the MRC model adjusting the cutoff to\n0.4 reduces half of the false negatives by trading of 25% of the TP.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "threshold = 0.35\ndf_CMRC = defDataFrame(\n    model=clf_CMRC, x_test=X_test, y_test=y_test, threshold=threshold\n)\nthreshold = 0.4\ndf_MRC = defDataFrame(model=clf_MRC, x_test=X_test,\n                      y_test=y_test, threshold=threshold)\npd.DataFrame(\n    classification_report(\n        df_CMRC.Real,\n        df_CMRC.Prediction,\n        target_names=[\"Survive\", \"Decease\"],\n        output_dict=True,\n    )\n).style.set_caption(\"Classification report CMRC \\n adjusted threshold\").format(\n    precision=3\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(\n    classification_report(\n        df_MRC.Real,\n        df_MRC.Prediction,\n        target_names=[\"Survive\", \"Decease\"],\n        output_dict=True,\n    )\n).style.set_caption(\"Classification report MRC \\n adjusted threshold\").format(\n    precision=3\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results:\nComparing the outputs of this example we can determine that MRCs work\nsignificantly well for estimating the outcome of COVID-19 patients at\nhospital triage.\n\nFurthermore, the CMRC model with threhsold feature mapping has shown a great\nperformance both for classifying and for estimating conditional probabilities\nFinally we have seen how to select the cut-off values based on data\nvisualization with beeswarm-boxplots to increase the recall in the desired\nclass.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}