{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nAn example of creating you own custom feature mappings\n\nIn this example, I am extending the Phi parent class\naccording to the needs of the mappings.\nYou can choose the best feature mapping class for extension\naccording to your requirements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.utils import check_array\n\nfrom MRCpy import MRC, CMRC\nfrom MRCpy.phi import *\n\n# Custom phi example: Generating the linear kernel\n# modified by multiplying a constant\n\n\nclass myPhi(BasePhi):\n\n    \"\"\"\n    This constructor is by default present in the parent Phi class.\n    So, no need to redefine this constructor\n    unless you need any extra parameters from the user.\n    In our example here, we don't actually need this\n    as we are not using any extra parameters here\n    but it is defined here as an example.\n    Removing this constructor doesn't have any affect on the performance.\n    \"\"\"\n    def __init__(self, n_classes, b=5):\n        # Calling the parent constructor.\n        # It is always better convention to call the parent constructor\n        # for primary variables initialization.\n        super().__init__(n_classes)\n\n        # Define any extra parameters for your own features\n        # Example : self.add_intercept = True/False\n\n    def fit(self, X, Y=None):\n        \"\"\"\n        Fit any extra parameter for your feature mappings\n        and set the length of the feature mapping.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_dimensions)\n            Unlabeled training instances\n            used to learn the feature configurations\n\n        Y : array-like of shape (n_samples,), default=None\n            Labels corresponding to the unlabeled instances.\n        \"\"\"\n\n        # Check if the array is 2D numpy matrix or not.\n        # X is expected to be a numpy 2D matrix.\n        X = check_array(X, accept_sparse=True)\n\n        d = X.shape[1]\n\n        # Defining the length of the phi\n\n        # Here we define the total length of the phi feature vector\n        # given by the class variable self.len_\n        # In this case, the kernel is linear,\n        # so the length of the kernel is d (Number of dimension of X)\n        # added by 1 (for the intercept).\n        # We one-hot encoded feature mapping\n        # so we multiply by number of classes for total length\n        self.len_ = (d + 1) * self.n_classes\n\n        # A class variable. You need set it true\n        # after you fit your feature mapping functions\n        self.is_fitted_ = True\n\n        # Return the fitted feature mapping instance\n        return self\n\n    def transform(self, X):\n\n        \"\"\"\n        Transform the given instances to the principal features if any.\n        No need to give definition for this function\n        if you are not calling it in the eval function.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_dimensions)\n            Unlabeled training instances.\n\n        Returns\n        -------\n        X_feat : array-like of shape (n_samples, n_features)\n            Transformed features from the given instances i.e.,\n            the instances itself.\n\n        \"\"\"\n\n        # We want to use the linear kernel feature mapping (i.e., X itself)\n        # and transform it by multiplying by a factor 2\n        # Note: This is just an example of building custom feature mappings,\n        #       so the results after using this feature mappings\n        #       might not be satisfactory\n        X_feat = X * 2\n\n        # Return the features\n        return X_feat\n\n    def eval(self, X, Y=None):\n\n        \"\"\"\n        Computes the complete feature mapping vector\n        corresponding to instance X.\n        X can be a matrix in which case\n        the function returns a matrix in which\n        the rows represent the complete feature mapping vector\n        corresponding to each instance.\n\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_dimensions)\n            Unlabeled training instances for developing the feature matrix\n\n        Y : array-like of shape (n_samples,), default=None\n            Labels corresponding to the unlabeled training instances\n\n        Returns\n        -------\n        phi : array-like of shape (n_samples, n_classes, n_features*n_classes)\n            Matrix containing the complete feature vector as rows\n            corresponding to each of the instance.\n            In case of one-hot encoding, the feature mappings are given by\n            calling the transform function to get the principal features and\n            then appending zeros for the one-hot encoding.\n            In case Y is given, the encoding for each instance are calculated\n            corresponding to those labels.\n        \"\"\"\n        # Here in this example,\n        # we want to use the one-hot encoded feature mappings.\n        # So, we call the parent class eval function\n        # which does the one-hot encoding by default\n        # and also adds the intercept corresponding to each class\n        return super().eval(X, Y)\n\n        # In case you don't want the one-hot encoding,\n        # you have to define you own eval function\n        # without calling the parent class eval function.\n\n\nif __name__ == '__main__':\n\n    # Loading the dataset\n    X, Y = load_iris(return_X_y=True)\n    r = len(np.unique(Y))\n\n    X = np.asarray([[1,2,3],[3,4,5],[8,9,5]])\n\n    n = X.shape[0]\n    y = np.asarray([0,1,2])\n    r=2\n\n    print('The instances are : ', X)\n    print('The labels are : ', y)\n\n    # Creating the custom phi object\n    # myphi = myPhi(n_classes=r)\n    # phi1 = BasePhi(n_classes=2, fit_intercept=False).fit(X,y)\n\n    # print(phi1.eval_x(np.repeat(X,r,axis=0), np.tile(np.arange(r), X.shape[0])))\n    # phi = phi1.eval_x(np.repeat(X,r,axis=0), np.tile(np.arange(r), X.shape[0]))\n    # print(phi.shape)\n    # phi = np.reshape(phi, (n, r, phi1.len_))\n    # print(phi)\n\n    # Fit the MRC model with the custom phi\n    clf = CMRC(phi='linear', fit_intercept=False).fit(X, y)\n\n    print(clf.phi.eval_x(X))\n    # Prediction\n    # print('\\n\\nThe predicted values for the first 3 instances are : ')\n    # print(clf.predict(X[:3, :]))\n\n    # Predicted probabilities\n    # print('\\n\\nThe predicted probabilities for the first 3 instances are : ')\n    # print(clf.predict_proba(X[:3, :]))\n\n    # Accuracy/Score of the model\n    # print('\\n\\nThe score is : ')\n    # print(clf.score(X, Y))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}