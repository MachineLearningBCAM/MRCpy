
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_example1.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_example1.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_example1.py:


.. _ex1:

Example: Use of MRC with different settings
===========

Example of using MRC with some of the common classification datasets with different
losses and feature mappings settings. We load the different datasets and use 10-Fold 
Cross-Validation to generate the partitions for train and test. We separate 1 partition
each time for testing and use the others for training. On each iteration we calculate
the classification error as well as the upper and lower bounds for the error. We also
calculate the mean training time.

You can check a more elaborated example in :ref:`ex_comp`.

.. GENERATED FROM PYTHON SOURCE LINES 18-121




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    *** Example (MRC with default constraints) *** 


    1. Using 0-1 loss and relu feature mapping 


     ############## 
     mammographic n= 961 , d= 5, cardY= 2
     error= : 0.18104596219931274 +/- 0.030155796025062662
     upper= 0.21562381798505958
     lower= 0.17179206690935778
     avg_train_time= : 2.7614378213882445 secs
     ############## 

     ############## 
     haberman n= 306 , d= 3, cardY= 2
     error= : 0.26838709677419353 +/- 0.04098509496168556
     upper= 0.25587309151465093
     lower= 0.21367449451838688
     avg_train_time= : 1.285470962524414 secs
     ############## 

     ############## 
     indian_liver n= 583 , d= 10, cardY= 2
     error= : 0.2898597311513735 +/- 0.011299355253418256
     upper= 0.2891465954024749
     lower= 0.27179973797079515
     avg_train_time= : 1.911133313179016 secs
     ############## 

     ############## 
     diabetes n= 768 , d= 8, cardY= 2
     error= : 0.24475393028024603 +/- 0.044905828380155596
     upper= 0.27254042514432225
     lower= 0.21890982245379842
     avg_train_time= : 3.18976686000824 secs
     ############## 

     ############## 
     credit n= 690 , d= 15, cardY= 2
     error= : 0.1405797101449275 +/- 0.03669272145267294
     upper= 0.17828699811794385
     lower= 0.10931790187765264
     avg_train_time= : 2.171911334991455 secs
     ############## 

    2. Using log loss and relu feature mapping 


     ############## 
     mammographic n= 961 , d= 5, cardY= 2
     error= : 0.1727233676975945 +/- 0.043440298168131546
     upper= 0.5183254983162376
     lower= 0.37670666902079813
     avg_train_time= : 9.495104575157166 secs
     ############## 

     ############## 
     haberman n= 306 , d= 3, cardY= 2
     error= : 0.25849462365591397 +/- 0.02604815129114169
     upper= 0.5694205122293716
     lower= 0.4589318373531638
     avg_train_time= : 3.942916250228882 secs
     ############## 

     ############## 
     indian_liver n= 583 , d= 10, cardY= 2
     error= : 0.2898597311513734 +/- 0.008259685044740707
     upper= 0.6012817789503437
     lower= 0.5396964913284981
     avg_train_time= : 7.310282635688782 secs
     ############## 

     ############## 
     diabetes n= 768 , d= 8, cardY= 2
     error= : 0.2630041011619959 +/- 0.037558273133120205
     upper= 0.5878030888325628
     lower= 0.46809074227515135
     avg_train_time= : 10.55997018814087 secs
     ############## 

     ############## 
     credit n= 690 , d= 15, cardY= 2
     error= : 0.14492753623188406 +/- 0.039953764498812244
     upper= 0.46732603037884096
     lower= 0.29597619054089835
     avg_train_time= : 7.616873669624328 secs
     ############## 







|

.. code-block:: default


    import time

    import numpy as np
    from sklearn import preprocessing
    from sklearn.model_selection import StratifiedKFold

    from MRCpy import MRC
    # Import the datasets
    from MRCpy.datasets import *

    # Data sets
    loaders = [load_mammographic, load_haberman, load_indian_liver,
               load_diabetes, load_credit]
    dataName = ["mammographic", "haberman", "indian_liver",
                "diabetes", "credit"]


    def runMRC(phi, loss):

        res_mean = np.zeros(len(dataName))
        res_std = np.zeros(len(dataName))

        # We fix the random seed to that the stratified kfold performed
        # is the same through the different executions
        random_seed = 0

        # Iterate through each of the dataset and fit the MRC classfier.
        for j, load in enumerate(loaders):

            # Loading the dataset
            X, Y = load(return_X_y=True)
            r = len(np.unique(Y))
            n, d = X.shape

            # Print the dataset name
            print(" ############## \n " + dataName[j] + " n= " + str(n) +
                  " , d= " + str(d) + ", cardY= " + str(r))

            clf = MRC(phi=phi, loss=loss, solver='MOSEK',
                      use_cvx=True, max_iters=10000, s=0.3)

            # Generate the partitions of the stratified cross-validation
            cv = StratifiedKFold(n_splits=10, random_state=random_seed,
                                 shuffle=True)

            cvError = list()
            auxTime = 0
            upper = 0
            lower = 0

            # Paired and stratified cross-validation
            for train_index, test_index in cv.split(X, Y):

                X_train, X_test = X[train_index], X[test_index]
                y_train, y_test = Y[train_index], Y[test_index]

                # Normalizing the data
                std_scale = preprocessing.StandardScaler().fit(X_train, y_train)
                X_train = std_scale.transform(X_train)
                X_test = std_scale.transform(X_test)

                # Save start time for computing training time
                startTime = time.time()

                # Train the model and save the upper and lower bounds
                clf.fit(X_train, y_train)
                upper += clf.get_upper_bound()
                lower += clf.get_lower_bound()

                # Save the training time
                auxTime += time.time() - startTime

                # Predict the class for test instances
                y_pred = clf.predict(X_test)

                # Calculate the error made by MRC classificator
                cvError.append(np.average(y_pred != y_test))

            res_mean[j] = np.average(cvError)
            res_std[j] = np.std(cvError)

            # Calculating the mean upper and lower bound and training time
            upper = upper / 10
            lower = lower / 10
            auxTime = auxTime / 10

            print(" error= " + ": " + str(res_mean[j]) + " +/- " +
                  str(res_std[j]))
            print(" upper= " + str(upper) + "\n lower= " + str(lower) +
                  "\n avg_train_time= " + ": " + str(auxTime) + ' secs' +
                  "\n ############## \n")


    if __name__ == '__main__':

        print('*** Example (MRC with default constraints) *** \n\n')

        print('1. Using 0-1 loss and relu feature mapping \n\n')
        runMRC(phi='relu', loss='0-1')

        print('2. Using log loss and relu feature mapping \n\n')
        runMRC(phi='relu', loss='log')


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 8 minutes  22.845 seconds)


.. _sphx_glr_download_auto_examples_plot_example1.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_example1.py <plot_example1.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_example1.ipynb <plot_example1.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
