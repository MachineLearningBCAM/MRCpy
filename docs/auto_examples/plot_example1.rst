
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_example1.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_example1.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_example1.py:


.. _ex1:

Example: Use of MRC with different settings
===========

Example of using MRC with some of the common classification datasets with
different losses and feature mappings settings. We load the different datasets
and use 10-Fold Cross-Validation to generate the partitions for train and test.
We separate 1 partition each time for testing and use the others for training.
On each iteration we calculate the classification error as well as the upper
and lower bounds for the error. We also
calculate the mean training time.

You can check a more elaborated example in :ref:`ex_comp`.

.. GENERATED FROM PYTHON SOURCE LINES 19-122




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    *** Example (MRC with default constraints) *** 


    1. Using 0-1 loss and relu feature mapping 


     ############## 
     mammographic n= 961 , d= 5, cardY= 2
     error= : 0.18940077319587628 +/- 0.026770683831872446
     upper= 0.2135523103127598
     lower= 0.16793595166653397
     avg_train_time= : 3.436056303977966 secs
     ############## 

     ############## 
     haberman n= 306 , d= 3, cardY= 2
     error= : 0.26838709677419353 +/- 0.03799276391418881
     upper= 0.25495414926908844
     lower= 0.21401659628557143
     avg_train_time= : 1.6984854698181153 secs
     ############## 

     ############## 
     indian_liver n= 583 , d= 10, cardY= 2
     error= : 0.29333722969023956 +/- 0.012489267459210357
     upper= 0.28866521965588743
     lower= 0.2694775557111216
     avg_train_time= : 2.5528562307357787 secs
     ############## 

     ############## 
     diabetes n= 768 , d= 8, cardY= 2
     error= : 0.2500341763499658 +/- 0.046354823640713895
     upper= 0.2708744209035068
     lower= 0.2198047968879444
     avg_train_time= : 4.118932461738586 secs
     ############## 

     ############## 
     credit n= 690 , d= 15, cardY= 2
     error= : 0.14202898550724635 +/- 0.03931785497463923
     upper= 0.17968010136848855
     lower= 0.10765359782681076
     avg_train_time= : 3.0744213342666624 secs
     ############## 

    2. Using log loss and relu feature mapping 


     ############## 
     mammographic n= 961 , d= 5, cardY= 2
     error= : 0.19146262886597937 +/- 0.03162954911115647
     upper= 0.5192606083022867
     lower= 0.3746550365772805
     avg_train_time= : 10.030834507942199 secs
     ############## 

     ############## 
     haberman n= 306 , d= 3, cardY= 2
     error= : 0.25849462365591397 +/- 0.02604815129114169
     upper= 0.5687735358724942
     lower= 0.45985098876574
     avg_train_time= : 5.63266954421997 secs
     ############## 

     ############## 
     indian_liver n= 583 , d= 10, cardY= 2
     error= : 0.2863822326125073 +/- 0.02212139262007088
     upper= 0.6007379208410979
     lower= 0.5330366145676775
     avg_train_time= : 9.262992525100708 secs
     ############## 

     ############## 
     diabetes n= 768 , d= 8, cardY= 2
     error= : 0.25786056049213946 +/- 0.0388581492092039
     upper= 0.5840184826432231
     lower= 0.4618767576475289
     avg_train_time= : 12.99996201992035 secs
     ############## 

     ############## 
     credit n= 690 , d= 15, cardY= 2
     error= : 0.14492753623188406 +/- 0.03995376449881224
     upper= 0.47152244315779585
     lower= 0.29770190730943635
     avg_train_time= : 9.140479969978333 secs
     ############## 







|

.. code-block:: default


    import time

    import numpy as np
    from sklearn import preprocessing
    from sklearn.model_selection import StratifiedKFold

    from MRCpy import MRC
    # Import the datasets
    from MRCpy.datasets import *

    # Data sets
    loaders = [load_mammographic, load_haberman, load_indian_liver,
               load_diabetes, load_credit]
    dataName = ["mammographic", "haberman", "indian_liver",
                "diabetes", "credit"]


    def runMRC(phi, loss):

        res_mean = np.zeros(len(dataName))
        res_std = np.zeros(len(dataName))

        # We fix the random seed to that the stratified kfold performed
        # is the same through the different executions
        random_seed = 0

        # Iterate through each of the dataset and fit the MRC classfier.
        for j, load in enumerate(loaders):

            # Loading the dataset
            X, Y = load(return_X_y=True)
            r = len(np.unique(Y))
            n, d = X.shape

            # Print the dataset name
            print(" ############## \n " + dataName[j] + " n= " + str(n) +
                  " , d= " + str(d) + ", cardY= " + str(r))

            clf = MRC(phi=phi, loss=loss, solver='MOSEK',
                      use_cvx=True, max_iters=10000, s=0.3)

            # Generate the partitions of the stratified cross-validation
            cv = StratifiedKFold(n_splits=10, random_state=random_seed,
                                 shuffle=True)

            cvError = list()
            auxTime = 0
            upper = 0
            lower = 0

            # Paired and stratified cross-validation
            for train_index, test_index in cv.split(X, Y):

                X_train, X_test = X[train_index], X[test_index]
                y_train, y_test = Y[train_index], Y[test_index]

                # Normalizing the data
                std_scale = preprocessing.StandardScaler().fit(X_train, y_train)
                X_train = std_scale.transform(X_train)
                X_test = std_scale.transform(X_test)

                # Save start time for computing training time
                startTime = time.time()

                # Train the model and save the upper and lower bounds
                clf.fit(X_train, y_train)
                upper += clf.get_upper_bound()
                lower += clf.get_lower_bound()

                # Save the training time
                auxTime += time.time() - startTime

                # Predict the class for test instances
                y_pred = clf.predict(X_test)

                # Calculate the error made by MRC classificator
                cvError.append(np.average(y_pred != y_test))

            res_mean[j] = np.average(cvError)
            res_std[j] = np.std(cvError)

            # Calculating the mean upper and lower bound and training time
            upper = upper / 10
            lower = lower / 10
            auxTime = auxTime / 10

            print(" error= " + ": " + str(res_mean[j]) + " +/- " +
                  str(res_std[j]))
            print(" upper= " + str(upper) + "\n lower= " + str(lower) +
                  "\n avg_train_time= " + ": " + str(auxTime) + ' secs' +
                  "\n ############## \n")


    if __name__ == '__main__':

        print('*** Example (MRC with default constraints) *** \n\n')

        print('1. Using 0-1 loss and relu feature mapping \n\n')
        runMRC(phi='relu', loss='0-1')

        print('2. Using log loss and relu feature mapping \n\n')
        runMRC(phi='relu', loss='log')


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 10 minutes  20.218 seconds)


.. _sphx_glr_download_auto_examples_plot_example1.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_example1.py <plot_example1.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_example1.ipynb <plot_example1.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
