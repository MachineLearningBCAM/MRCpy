
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/plot_example1.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_example1.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_plot_example1.py:


.. _ex1:

Example: Use of MRC with different settings
===========

Example of using MRC with some of the common classification datasets with
different losses and feature mappings settings. We load the different datasets
and use 10-Fold Cross-Validation to generate the partitions for train and test.
We separate 1 partition each time for testing and use the others for training.
On each iteration we calculate the classification error as well as the upper
and lower bounds for the error. We also
calculate the mean training time.

You can check a more elaborated example in :ref:`ex_comp`.

.. GENERATED FROM PYTHON SOURCE LINES 19-122




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    *** Example (MRC with default constraints) *** 


    1. Using 0-1 loss and relu feature mapping 


     ############## 
     mammographic n= 961 , d= 5, cardY= 2
     error= : 0.1841494845360825 +/- 0.03402812948606356
     upper= 0.21270318025123047
     lower= 0.1672240190239284
     avg_train_time= : 2.7843384742736816 secs
     ############## 

     ############## 
     haberman n= 306 , d= 3, cardY= 2
     error= : 0.2682795698924731 +/- 0.03106484924098504
     upper= 0.25536977896928315
     lower= 0.2093921071258468
     avg_train_time= : 1.3245277166366578 secs
     ############## 

     ############## 
     indian_liver n= 583 , d= 10, cardY= 2
     error= : 0.2864699006428989 +/- 0.008271669534078716
     upper= 0.28886391741934564
     lower= 0.2717994106295661
     avg_train_time= : 2.070624828338623 secs
     ############## 

     ############## 
     diabetes n= 768 , d= 8, cardY= 2
     error= : 0.2538961038961039 +/- 0.045046762936155074
     upper= 0.2720776464176731
     lower= 0.21973310843336416
     avg_train_time= : 3.199790596961975 secs
     ############## 

     ############## 
     credit n= 690 , d= 15, cardY= 2
     error= : 0.14347826086956522 +/- 0.04222841241835787
     upper= 0.17801406338999765
     lower= 0.11014872295271867
     avg_train_time= : 2.234352874755859 secs
     ############## 

    2. Using log loss and relu feature mapping 


     ############## 
     mammographic n= 961 , d= 5, cardY= 2
     error= : 0.1800150343642612 +/- 0.024177260112934106
     upper= 0.5191399309989541
     lower= 0.3735496866650937
     avg_train_time= : 10.116348099708556 secs
     ############## 

     ############## 
     haberman n= 306 , d= 3, cardY= 2
     error= : 0.25849462365591397 +/- 0.02604815129114169
     upper= 0.5672346090254049
     lower= 0.4559778816972281
     avg_train_time= : 4.107697057723999 secs
     ############## 

     ############## 
     indian_liver n= 583 , d= 10, cardY= 2
     error= : 0.2915838690824079 +/- 0.015002251736270412
     upper= 0.5986629738695497
     lower= 0.5211041997945969
     avg_train_time= : 8.577530074119569 secs
     ############## 

     ############## 
     diabetes n= 768 , d= 8, cardY= 2
     error= : 0.23174982911825018 +/- 0.03322266438656587
     upper= 0.5844220354251606
     lower= 0.4626785143884944
     avg_train_time= : 11.35982367992401 secs
     ############## 

     ############## 
     credit n= 690 , d= 15, cardY= 2
     error= : 0.14347826086956522 +/- 0.0391304347826087
     upper= 0.4689327591152871
     lower= 0.2979467646144761
     avg_train_time= : 8.245241713523864 secs
     ############## 







|

.. code-block:: default


    import time

    import numpy as np
    from sklearn import preprocessing
    from sklearn.model_selection import StratifiedKFold

    from MRCpy import MRC
    # Import the datasets
    from MRCpy.datasets import *

    # Data sets
    loaders = [load_mammographic, load_haberman, load_indian_liver,
               load_diabetes, load_credit]
    dataName = ["mammographic", "haberman", "indian_liver",
                "diabetes", "credit"]


    def runMRC(phi, loss):

        res_mean = np.zeros(len(dataName))
        res_std = np.zeros(len(dataName))

        # We fix the random seed to that the stratified kfold performed
        # is the same through the different executions
        random_seed = 0

        # Iterate through each of the dataset and fit the MRC classfier.
        for j, load in enumerate(loaders):

            # Loading the dataset
            X, Y = load(return_X_y=True)
            r = len(np.unique(Y))
            n, d = X.shape

            # Print the dataset name
            print(" ############## \n " + dataName[j] + " n= " + str(n) +
                  " , d= " + str(d) + ", cardY= " + str(r))

            clf = MRC(phi=phi, loss=loss, solver='MOSEK',
                      use_cvx=True, max_iters=10000, s=0.3)

            # Generate the partitions of the stratified cross-validation
            cv = StratifiedKFold(n_splits=10, random_state=random_seed,
                                 shuffle=True)

            cvError = list()
            auxTime = 0
            upper = 0
            lower = 0

            # Paired and stratified cross-validation
            for train_index, test_index in cv.split(X, Y):

                X_train, X_test = X[train_index], X[test_index]
                y_train, y_test = Y[train_index], Y[test_index]

                # Normalizing the data
                std_scale = preprocessing.StandardScaler().fit(X_train, y_train)
                X_train = std_scale.transform(X_train)
                X_test = std_scale.transform(X_test)

                # Save start time for computing training time
                startTime = time.time()

                # Train the model and save the upper and lower bounds
                clf.fit(X_train, y_train)
                upper += clf.get_upper_bound()
                lower += clf.get_lower_bound()

                # Save the training time
                auxTime += time.time() - startTime

                # Predict the class for test instances
                y_pred = clf.predict(X_test)

                # Calculate the error made by MRC classificator
                cvError.append(np.average(y_pred != y_test))

            res_mean[j] = np.average(cvError)
            res_std[j] = np.std(cvError)

            # Calculating the mean upper and lower bound and training time
            upper = upper / 10
            lower = lower / 10
            auxTime = auxTime / 10

            print(" error= " + ": " + str(res_mean[j]) + " +/- " +
                  str(res_std[j]))
            print(" upper= " + str(upper) + "\n lower= " + str(lower) +
                  "\n avg_train_time= " + ": " + str(auxTime) + ' secs' +
                  "\n ############## \n")


    if __name__ == '__main__':

        print('*** Example (MRC with default constraints) *** \n\n')

        print('1. Using 0-1 loss and relu feature mapping \n\n')
        runMRC(phi='relu', loss='0-1')

        print('2. Using log loss and relu feature mapping \n\n')
        runMRC(phi='relu', loss='log')


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 9 minutes  0.659 seconds)


.. _sphx_glr_download_auto_examples_plot_example1.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_example1.py <plot_example1.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_example1.ipynb <plot_example1.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
