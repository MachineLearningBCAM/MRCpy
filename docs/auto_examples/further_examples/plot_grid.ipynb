{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Hyperparameter Tuning: Upper Bound vs Cross-Validation\n\nExample of how to use the Upper Bounds provided by the `MRC` method in the\n`MRCpy` library for hyperparameter tuning and comparison to Cross-Validation.\nWe will see that using the Upper Bound gets similar performances to\nCross-Validation but being four times faster.\n\nWe are using '0-1' loss and `RandomFourierPhi`\nmap (`phi='fourier'`). We are going to tune the scaling parameter\n`sigma` and the regularization parameter `s` of the\nfeature mapping using a random grid. We will used the usual method\n`RandomizedSearchCV<https://scikit-learn.org/stable/modules/\ngenerated/sklearn.model_selection.RandomizedSearchCV.html>`\nfrom `scikit-learn`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Import needed modules\nimport random\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom scipy.spatial import distance\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\n\nfrom MRCpy import MRC\nfrom MRCpy.datasets import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Grid using Upper Bound parameter\nWe select random `n_iter` random set of values for the parameters to tune in\na given range and select the pair of parameters which minimizes the upper\nbound provided by the MRC method.\nOn each repetition we calculate and store the upper bound for each possible\nvalue of sigma.\nThe parameter `n_iter` means the amount of randomly selected vectors for the\nparameters to\ntune are chosen. We are selecting `n_iter = 10` because it is the default\nvalue for the RandomGridCV method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run_RandomGridUpper(X_train, Y_train, X_test, Y_test, sigma_ini, sigma_fin,\n                        s_ini, s_fin):\n    n_iter = 10\n    startTime = time.time()\n    sigma_id = [(sigma_fin - sigma_ini) * random.random() + sigma_ini\n                for i in range(n_iter)]\n    s_id = [(s_fin - s_ini) * random.random() + s_ini for i in range(n_iter)]\n    upps = np.zeros(n_iter)\n\n    for i in range(n_iter):\n        clf = MRC(phi='fourier', sigma=sigma_id[i], s=s_id[i], random_state=0,\n                  deterministic=False, use_cvx=True, solver='MOSEK')\n        clf.fit(X_train, Y_train)\n        upps[i] = clf.get_upper_bound()\n\n    min_upp = np.min(upps)\n    best_sigma = sigma_id[np.argmin(upps)]\n    best_s = s_id[np.argmin(upps)]\n    clf = MRC(phi='fourier', sigma=best_sigma, s=best_s, random_state=0,\n              deterministic=False, use_cvx=True, solver='MOSEK')\n    clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    best_err = np.average(Y_pred != Y_test)\n    totalTime = time.time() - startTime\n\n    return {'upper': min_upp, 's': best_s,\n            'sigma': best_sigma, 'time': totalTime, 'error': best_err}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RandomGridCV\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run_RandomGridCV(X_train, Y_train, X_test, Y_test, sigma_ini, sigma_fin,\n                     s_ini, s_fin):\n    n_iter = 10\n    startTime = time.time()\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25,\n                                                        random_state=rep)\n    # Normalizing the data\n    std_scale = preprocessing.StandardScaler().fit(X_train, Y_train)\n    X_train = std_scale.transform(X_train)\n    X_test = std_scale.transform(X_test)\n\n    sigma_values = np.linspace(sigma_ini, sigma_fin, num=5000)\n    s_values = np.linspace(s_ini, s_fin, num=5000)\n    param = {'sigma': sigma_values, 's': s_values}\n\n    mrc = MRC(phi='fourier', random_state=0, deterministic=False, use_cvx=True,\n              solver='MOSEK')\n    clf = RandomizedSearchCV(mrc, param, random_state=0, n_iter=n_iter)\n    clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    error = np.average(Y_pred != Y_test)\n\n    totalTime = time.time() - startTime\n\n    return {'upper': clf.best_estimator_.get_upper_bound(),\n            's': clf.best_estimator_.s,\n            'sigma': clf.best_estimator_.phi.sigma_val,\n            'time': totalTime, 'error': error}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison\nWe are performing both of the previous methods for hyperparameter tuning\nover a set of different datasets and comparing the performances.\nBefore calling them, we set a range of values for the hyperpatameters.\nAn intuituve way of choosing sigma is to choose values in the range of the\ndistance among the pairs of instances in the trainign set `X_train`.\nEmpirical knowledge tells us that best values for s use to be around\n0.3 and 0.6.\n\nWe repeat these processes several times to make sure performances do not\nrely heavily on the train_test_split selected.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_table(df, title, color):\n    fig, ax = plt.subplots()\n    # hide axes\n    fig.patch.set_visible(False)\n    ax.axis('off')\n    ax.axis('tight')\n    t = ax.table(cellText=df.values, colLabels=df.columns, loc='center',\n                 colColours=color, cellColours=[color] * len(df))\n    t.auto_set_font_size(False)\n    t.set_fontsize(8)\n    t.auto_set_column_width(col=list(range(len(df.columns))))\n    fig.tight_layout()\n    plt.title(title)\n    plt.show()\n\n\nloaders = [load_mammographic, load_haberman, load_indian_liver,\n           load_diabetes, load_credit]\ndataNameList = [\"mammographic\", \"haberman\", \"indian_liver\",\n                \"diabetes\", \"credit\"]\n\ndfCV = pd.DataFrame()\ndfUpper = pd.DataFrame()\nf = '%1.3g'  # format\nfor j, load in enumerate(loaders):\n\n    # Loading the dataset\n    X, Y = load()\n    dataName = dataNameList[j]\n\n    # In order to avoid the possible bias made by the choice of the train-test\n    # split, we do this process several (20) times and average the\n    # obtained results\n    dfCV_aux = pd.DataFrame()\n    dfUpper_aux = pd.DataFrame()\n    for rep in range(10):\n        X_train, X_test, Y_train, Y_test = \\\n            train_test_split(X, Y, test_size=0.25, random_state=rep)\n        # Normalizing the data\n        std_scale = preprocessing.StandardScaler().fit(X_train, Y_train)\n        X_train = std_scale.transform(X_train)\n        X_test = std_scale.transform(X_test)\n\n        # Select an appropiate range for sigma\n        d = np.triu(distance.cdist(X_train, X_train)).flatten()\n        d = d[d != 0]\n        d.sort()\n        sigma_ini = d[int(len(d) * 0.1)]\n        sigma_fin = d[int(len(d) * 0.3)]\n        s_ini = 0.3\n        s_fin = 0.6\n\n        # We tune the parameters using both method and store the results\n        dfCV_aux = dfCV_aux.append(\n            run_RandomGridCV(X_train, Y_train, X_test, Y_test, sigma_ini,\n                             sigma_fin, s_ini, s_fin), ignore_index=True)\n        dfUpper_aux = dfUpper_aux.append(\n            run_RandomGridUpper(X_train, Y_train, X_test, Y_test, sigma_ini,\n                                sigma_fin, s_ini, s_fin), ignore_index=True)\n\n    # We save the mean results of the 20 repetitions\n    mean_err = f % np.mean(dfCV_aux['error']) + ' \u00b1 ' + \\\n        f % np.std(dfCV_aux['error'])\n    mean_sig = f % np.mean(dfCV_aux['sigma']) + ' \u00b1 ' + \\\n        f % np.std(dfCV_aux['sigma'])\n    mean_s = f % np.mean(dfCV_aux['s']) + ' \u00b1 ' + f % np.std(dfCV_aux['s'])\n    mean_time = f % np.mean(dfCV_aux['time']) + ' \u00b1 ' + \\\n        f % np.std(dfCV_aux['time'])\n    mean_upper = f % np.mean(dfCV_aux['upper']) + ' \u00b1 ' + \\\n        f % np.std(dfCV_aux['upper'])\n    dfCV = dfCV.append({'dataset': dataName, 'error': mean_err,\n                        'sigma': mean_sig, 's': mean_s,\n                        'upper': mean_upper,\n                        'time': mean_time}, ignore_index=True)\n    mean_err = f % np.mean(dfUpper_aux['error']) + ' \u00b1 ' + \\\n        f % np.std(dfUpper_aux['error'])\n    mean_sig = f % np.mean(dfUpper_aux['sigma']) + ' \u00b1 ' + \\\n        f % np.std(dfUpper_aux['sigma'])\n    mean_s = f % np.mean(dfUpper_aux['s']) + ' \u00b1 ' + \\\n        f % np.std(dfUpper_aux['s'])\n    mean_time = f % np.mean(dfUpper_aux['time']) + ' \u00b1 ' + \\\n        f % np.std(dfUpper_aux['time'])\n    mean_upper = f % np.mean(dfUpper_aux['upper']) + ' \u00b1 ' + \\\n        f % np.std(dfUpper_aux['upper'])\n    dfUpper = dfUpper.append({'dataset': dataName, 'error': mean_err,\n                              'sigma': mean_sig, 's': mean_s,\n                              'upper': mean_upper,\n                              'time': mean_time}, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dfCV.style.set_caption('RandomGridCV Results').set_properties(\n    **{'background-color': 'lightskyblue'}, subset=['error', 'time'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dfUpper.style.set_caption('RandomGridUpper Results').set_properties(\n    **{'background-color': 'lightskyblue'}, subset=['error', 'time'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results\nComparing the resulting tables above we notice that both methods:\nRandomGridCV and Random Grid using Upper bounds are really similar in\nperformance, one can do better than the other depending on the datasets but\nhave overall the same error range.\n\nFurthermore we can see how using the Upper bounds results in a great\nimprovement in the running time being around 4 times quicker than\nthe usual RandomGrid method.\n\nWe note that in every dataset the optimum value for the parameter s seems\nto be  always around 0.3, that is why this value has been chosen to be\nthe default value for the library.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}