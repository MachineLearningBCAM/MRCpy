{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# MRCs with Deep Neural Networks: Part I\nIn this example we will use a pretrained neural network to extract features\nof images in a dataset to train and test MRCs with these features in\n`feature_mrc`.\n\nWe are using `ResNet18 <https://pytorch.org/hub/pytorch_vision_resnet/>`_\npretrained model implementation in Pytorch library. Resnet models were proposed\nin \u201cDeep Residual Learning for Image Recognition\u201d. Here we are using the\nversion ResNet18 which contains 18 layers and it is pretrained over\n`ImageNet dataset<https://www.image-net.org/index.php>` which has 1000\ndifferent classes.\n\n.. seealso:: For more information about ResNet models refer to\n\n                [1] He, K., Zhang, X., Ren, S., & Sun, J. (2016).\n                Deep residual learning for image recognition.\n                In Proceedings of the IEEE conference on computer\n                vision and pattern recognition (pp. 770-778).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to Pretrained models, Transfer Learning and Feature Extraction\nDeep convolutional neural network models may take days or even weeks\nto train on very large datasets. A way to short-cut this process is\nto re-use the model weights from *pre-trained models* that were developed\nfor standard computer vision benchmark datasets, such as the\nImageNet image recognition tasks.\nTop performing models can be downloaded and used directly, or integrated\ninto a new model for your own computer vision problems.\n*Transfer learning* generally refers to a process where a model trained\non one problem is used in some way on a second related problem.\nAlternately, the pretrained models may be used as feature extraction models.\nHere, the output of the model from a layer prior to the output layer\nof the model is used as input to a new classifier model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load pretrained model and preprocess images\nFirstly, we load the pretrained model `torchvision.models.ResNet18` and\nwe take out the last layer in order to obtain the features. We call this\nfeature extraction model `resnet18_features`.\n\nIn the next lines we use `torchvision.transforms.Compose` to compose several\ntransforms together. In line [2] the input is resized to match its smaller\nedge to the given size, 256. That is, the image is resized mantaining\nthe aspect ratio and `min(height,width)=256`.\nIn line [3] we use the function `CenterCrop` to crop the given image\nat the center to `(224,224)`.  If image size is smaller than output\nsize along any edge, image is padded with 0 and then center cropped.\n\nIf you use smaller images, the kernels might not be able to extract the\nfeatures with the usual size, since they are smaller (ore larger),\nwhich may result in a difference in performance.\n\nFunction in line [4] converts a PIL Image\nor numpy.ndarray to tensor. Finally `Normalize` function (lines [5,6])\nnormalizes a tensor image with mean and standard deviation required by\nResNet18 method (check more in\n`pytorch ResNet18 doc<https://pytorch.org/hub/pytorch_vision_resnet/>`).\nGiven mean:\n`(mean[1],...,mean[n])` and std: `(std[1],..,std[n])` for `n` channels,\nthis transform will normalize each channel i.e.,\n`output[channel] = (input[channel] - mean[channel]) / std[channel]`\nYou can check more about `torchvision.transforms` in\n`pytorch docummentation<https://pytorch.org/vision/stable/transforms.html>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom os.path import join\n\nimport numpy as np\nimport tensorflow_datasets as tfds\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom PIL import Image\nfrom torchvision import transforms\n\nfrom MRCpy.datasets import load_yearbook_path\n\nresnet18 = models.resnet18(pretrained=True)\nfeatures_resnet18 = nn.Sequential(*(list(resnet18.children())[:-1]))\nfeatures_resnet18.eval()\n\ntransform = transforms.Compose(                              # [1]\n    [transforms.Resize(256),                                 # [2]\n     transforms.CenterCrop(224),                             # [3]\n     transforms.ToTensor(),                                  # [4]\n     transforms.Normalize(mean=[0.485, 0.456, 0.406],        # [5]\n                          std=[0.229, 0.224, 0.225])])       # [6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using tensorflow datasets: MNIST & Cats vs Dogs\nMNIST\n-----\nThe MNIST database of handwritten digits, available from\n`this page<http://yann.lecun.com/exdb/mnist/>`,\nhas a training set of 60000 examples, and a test set of 10000 examples. All\nimages have dimension (28,28,1) and they are greyscale. Tensorflow provides\nwith a convenient function to directly load this dataset into the scope\nwithout the need of downloading and storing the dataset locally, you can\ncheck more in `tensorflow documentation\n<https://www.tensorflow.org/datasets/catalog/mnist>`_.\nIt already provides with the train and test partitions. We load the dataset\nwith the function `tensorflow_datasets.load` and we specify\n`as_supervised=True` to indicate that we want to load the labels together\nwith the images and `with_info=True` will return the tuple\n`(tf.data.Dataset, tfds.core.DatasetInfo)`,\nthe latter containing the info associated with the builder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "[[ds_train, ds_test], ds_info] = tfds.load('mnist', split=['train', 'test'],\n                                           as_supervised=True, with_info=True)\n\ndf_train = tfds.as_dataframe(ds_train, ds_info)\ndf_test = tfds.as_dataframe(ds_test, ds_info)\n\nimages_train = df_train['image'].to_numpy()\nY_train = df_train['label'].to_numpy()\nimages_test = df_test['image'].to_numpy()\nY_test = df_test['label'].to_numpy()\n\nX_train = []\nX_test = []\n\n\nfor img_array in images_train:\n    # We convert the gray scale into RGB because it is what the model expect\n    img_array = np.repeat(img_array, 3, axis=-1)\n    img = Image.fromarray(img_array, mode='RGB').resize((224, 224))\n    img_t = transform(img)\n    batch_t = torch.unsqueeze(img_t, 0)\n    X_train.append(features_resnet18(batch_t).detach().numpy().flatten())\n\nfor img_array in images_test:\n    # We convert the gray scale into RGB because it is what the model expect\n    img_array = np.repeat(img_array, 3, axis=-1)\n    img = Image.fromarray(img_array, mode='RGB').resize((224, 224))\n    img_t = transform(img)\n    batch_t = torch.unsqueeze(img_t, 0)\n    X_test.append(features_resnet18(batch_t).detach().numpy().flatten())\n\nmnist_features_resnet18_train = np.concatenate(\n    (X_train, np.reshape(Y_train, (-1, 1))), axis=1)\n\nmnist_features_resnet18_test = np.concatenate(\n    (X_test, np.reshape(Y_test, (-1, 1))), axis=1)\n\nnp.savetxt('mnist_features_resnet18_train.csv', mnist_features_resnet18_train,\n           delimiter=',')\nnp.savetxt('mnist_features_resnet18_test.csv', mnist_features_resnet18_test,\n           delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cats vs Dogs\nCats vs dogs dataset is a database of 23262 RGB cats\nand dogs images released by Microsoft for the Asirra captcha (`homepage\n<https://www.microsoft.com/en-us/download/details.aspx?id=54765>`_).\nCats are labeled by 0 and dogs by 1 and there are 11658 and 11604 images\nof each class, respectively.\nIt is available in tensorflow datasets, you can check the details `here\n<https://www.tensorflow.org/datasets/catalog/cats_vs_dogs>`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "[ds, ds_info] = tfds.load('cats_vs_dogs', split='train',\n                          as_supervised=True, with_info=True)\n\ndf = tfds.as_dataframe(ds, ds_info)\nimages = df['image'].to_numpy()\nlabels = df['label'].to_numpy()\n\nX_features = []\nfor img_array in images:\n    img = Image.fromarray(img_array, mode='RGB')\n    img_t = transform(img)\n    batch_t = torch.unsqueeze(img_t, 0)\n    X_features.append(features_resnet18(batch_t).detach().numpy().flatten())\n\ncatsvsdogs_features_resnet18 = np.concatenate((X_features,\n                                               np.reshape(labels, (-1, 1))),\n                                              axis=1)\n\nnp.savetxt('catsvsdogs_features_resnet18.csv', catsvsdogs_features_resnet18,\n           delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using a local dataset: Yearbook Dataset\nIn this example, we are going to extract the features from a local dataset.\nWe will be using the Yearbook dataset which is a publicly-available dataset\nof 37,921 frontal-facing American high school yearbook portraits taken from\n1905 to 2013 labeled by gender.\nWe will consider binary classification labels identifying\nwhether the person on the image is a man or a woman.\n\n.. seealso:: More information about Yearbook dataset can be found in\n\n              [1] Ginosar, S., Rakelly, K., Sachs, S., Yin, B., & Efros,\n              A. A. (2015). A century of portraits: A visual historical\n              record of american high school yearbooks. In Proceedings of\n              the IEEE International Conference on Computer Vision Workshops\n              (pp. 1-7).\n\n              [2] Kumar, A., Ma, T., & Liang, P. (2020, November).\n              Understanding self-training for gradual domain adaptation.\n              In International Conference on Machine Learning\n              (pp. 5468-5479). PMLR.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We take paths and names from images from F (female) and M (male)\nfolder and merge them in a dataset ordered by date\n(as images name start by the year of the photo). We convert the labels into\n0 for F and 1 for M.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_path = load_yearbook_path()\nF_path = join(data_path, 'F')\nF = os.listdir(F_path)\nF = np.concatenate((np.reshape(F, (len(F), 1)), np.zeros((len(F), 1)),\n                    np.reshape([F_path + x for x in F], (len(F), 1))), axis=1)\nM_path = join(data_path, 'M')\nM = os.listdir(M_path)\nM = np.concatenate((np.reshape(M, (len(M), 1)), np.ones((len(M), 1)),\n                    np.reshape([M_path + x for x in M], (len(M), 1))), axis=1)\ndata = np.concatenate((F, M), axis=0)\ndata = data[np.argsort(data[:, 0])]\n\npaths = data[:, 2]\nY = data[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we load the images, transform them using the function `transform` we\ndefined above to make the image compatible with ResNet18. Lastly, we extract\nthe image features using `features_resnet18()` and we transform the output\nfeatures to a flat array that will be a new instance of our feature dataset.\nWe store this feature dataset extracted with resnet18 in a csv file that\nis available in the `dataset` folder of the MRCpy library.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_features = []\n\nfor img_path in paths:\n    img = Image.open(img_path)\n    img_t = transform(img)\n    batch_t = torch.unsqueeze(img_t, 0)\n    X_features.append(features_resnet18(batch_t).detach().numpy().flatten())\n\nyearbook_features_resnet18 = np.concatenate((X_features,\n                                             np.reshape(Y, (-1, 1))), axis=1)\nnp.savetxt('yearbook_features_resnet18.csv',\n           yearbook_features_resnet18, delimiter=',')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}