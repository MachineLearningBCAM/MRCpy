{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n\n# Example: Predicting COVID-19 patients outcome using MRCs in highly class imbalanced dataset.\n\nIn this example we will use `MRCpy.MRC` and `MRCpy.CMRC` to predict the outcome\nof a COVID-19 positive patient at the moment of hospital triage. This example uses a dataset  \nthat comprises different demographic variables and biomarkers of the patients \nand a binary outcome :attr:`Status` where :attr:`Status = 0` define the group\nof survivors and :attr:`Status = 1` determines a decease.\n\nThe data is provided by the `Covid Data Saves Lives <https://www.hmhospitales.com/coronavirus/covid-data-save-lives/english-version>`_ \ninitiative carried out by HM Hospitales with information of the first wave of \nthe COVID outbreak in Spanish hospitals. The data is available upon request through HM Hospitales `here <https://www.hmhospitales.com/coronavirus/covid-data-save-lives/english-version>`_ .\n\n.. seealso::    For more information about the dataset and the creation of a risk\n                indicator using Logistic regression refer to:\n                \n                [1] Ruben Arma\u00f1anzas et al. \u201cDerivation of a Cost-Sensitive COVID-19 \n                Mortality Risk Indicator Using a Multistart Framework\", in *2021 \n                IEEE International Conference on Bioinformatics and Biomedicine (BIBM)*, 2021, pp. \n                2179\u20132186. \n\n \n    \nFirst we will see how to deal with class imbalance when training a model using\nsyntethic minority over-sampling (SMOTE) techniques. Furthermore, we will compare\ntwo MRC with two state of the art machine learning models probability estimation . The \nselected models are :mod:`CMRC(phi = 'threshold' , loss = 'log')` & :mod:`MRC(phi = 'fourier' , loss = 'log')`\nfor  the group of MRCs and Logistic Regression (LR) & C-Support Vector Classifier(SVC)\nwith the implementation from `Scikit-Learn <https://scikit-learn.org/stable/#>`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Import needed modules\nimport pandas as pd\nimport numpy as np\nimport time\n\n# sklearn\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\n# SMOTE over-sampling \nfrom imblearn.over_sampling import SMOTENC\n\n#MRCpy\nfrom MRCpy import MRC,CMRC\n\n# Data visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## COVID dataset Loader:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def load_covid(norm = False, array = True ):\n    data_consensus = pd.read_csv(\"data/data_consensus.csv\", sep=\";\")\n    # rename variables\n    variable_dict = {'CD0000AGE': 'Age', 'CORE': 'PATIENT_ID', 'CT000000U': 'Urea', \n                     'CT00000BT': 'Bilirubin', 'CT00000NA': 'Sodium', \n                     'CT00000TP': 'Proth_time', 'CT0000COM': 'Com', \n                     'CT0000LDH': 'LDH', 'CT0000NEU': 'Neutrophils',\n                     'CT0000PCR': 'Pro_C_Rea', 'CT0000VCM': 'Med_corp_vol', \n                     'CT000APTT': 'Ceph_time', 'CT000CHCM': 'Mean_corp_Hgb', \n                     'CT000EOSP': 'Eosinophils%', 'CT000LEUC': 'Leukocytes', \n                     'CT000LINP': 'Lymphocytes%', 'CT000NEUP': 'Neutrophils%', \n                     'CT000PLAQ': 'Platelet_count', 'CTHSDXXRATE': 'Rate', \n                     'CTHSDXXSAT': 'Sat', 'ED0DISWHY': 'Status', \n                     'F_INGRESO/ADMISSION_D_ING/INPAT': 'Fecha_admision',\n                     'SEXO/SEX': 'Sexo'}\n    data_consensus = data_consensus.rename(columns=variable_dict)\n    if norm: # if we want the data standardised\n        x_consensus = data_consensus[data_consensus.columns.difference(['Status', 'PATIENT_ID'])][:]\n        std_scale = preprocessing.StandardScaler().fit(x_consensus)\n        x_consensus_std = std_scale.transform(x_consensus)\n        dataframex_consensus = pd.DataFrame(\n            x_consensus_std, columns=x_consensus.columns)\n        data_consensus.reset_index(drop=True, inplace=True)\n        data_consensus = pd.concat(\n            [dataframex_consensus, data_consensus[[\"Status\"]]], axis=1)\n\n    data_consensus = data_consensus[data_consensus.columns.difference(['PATIENT_ID'])]\n    X = data_consensus[data_consensus.columns.difference(['Status', 'PATIENT_ID'])]\n    y = data_consensus['Status']\n    if array: \n        X = X.to_numpy()\n        y = y.to_numpy()\n    return X,y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Addressing dataset imbalance with SMOTE\n-------------------------------- \nThe COVID dataset has a significant problem of class imbalance where the positive outcome\nhas a prevalence of 85% (1522) whilst the negative outcome  has only 276.\nIn this example oversampling will be used to add syintetic records to get an almost  \nbalanced dataset. :mod:`SMOTE` (Synthetic minority over sampling) is\na package that implements such oversampling.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X,y = load_covid(array = False)\ndescribed = X.describe(percentiles = [0.5]).round(2).transpose()[['count','mean','std']]\npd.DataFrame(y.value_counts().rename({0.0: 'Survive',1.0:'Decease'}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we create a set of cases syntehtically using 5 nearest neighbors until \nthe class imbalance is almost removed. For more information about :mod:`SMOTE` refer to it's\n`documentation <https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html>`_ .\nWe will use the method `SMOTE-NC` for numerical and categorical variables. \n\n.. seealso::    For more information about the SMOTE package refer to:\n\n               [2] Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16, 321-357. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We fit the data to the oversampler\nsmotefit = SMOTENC(sampling_strategy=0.75, categorical_features=[3])\nX_resampled, y_resampled = smotefit.fit_resample(X, y)\ndescribed_resample = X_resampled.describe(percentiles = [0.5]).round(2).transpose()[['count','mean','std']]\ndescribed_resample = described_resample .add_suffix('_SMT')\npd.concat([described, described_resample], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see how the distribution of the real data and the resampled data is different.\nHowever the distribution between classes is kept similar due to the creation of \nthe synthetic cases through 5 nearest neighbors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(y_resampled.value_counts().rename({0.0: 'Survive',1.0:'Decease'}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Probability estimation\nIn this section we will estimate the conditional probabilities and analyse the \ndistribution of the probabilities depending on the real outcome .\nThe probability estimation is better when using :mod:`loss = log`. \nWe use :mod:`CMRC(phi = 'threshold', loss = 'log')` and  :mod:`MRC(phi = 'fourier' , loss = 'log'`.\nWe will then compare these MRCs with SVC and LR with default parameters.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load classification function:\nThese function classify each of the cases in their correspondent\nconfusion matrix's category. It also allows to set the desired cut-off\nfor the predictions.  \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def defDataFrame(model, x_test, y_test,threshold = 0.5):\n    \"\"\"\n    Takes x,y test and train and a fitted model and computes the probabilities to then classify in TP,TN , FP , FN \n    \"\"\"\n    if 'predict_proba' in dir (model):\n        probabilities = model.predict_proba(x_test)[:,1]\n        predictions = [1 if i >threshold else 0 for i in probabilities]\n        df = pd.DataFrame({'Real': y_test.tolist(),\n                       'Prediction': predictions,\n                      'Probabilities': probabilities.tolist()})\n    else:\n        df = pd.DataFrame({'Real': y_test.tolist(),\n                       'Prediction': model.predict(x_test)})    \n    conditions = [\n    (df['Real']==1) & (df['Prediction']==1),\n    (df['Real']==1) & (df['Prediction']==0),\n    (df['Real']==0) & (df['Prediction']==0),\n    (df['Real']==0) & (df['Prediction']==1),\n    ]\n    choices = [\"True Positive\",\"False Negative\" , \"True Negative\", \"False Positive\"]\n    df['Category'] = np.select(conditions, choices, default='No')\n    df.sort_index(inplace = True)\n    df.sort_values(by = 'Category',ascending = False, inplace = True)     \n    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train models:\n~~~~~~~~~~~~~~~~~~~~   \nWe will train the models with 80% of the data and then test with the other \n20% selected randomly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=1)\n\nclf_MRC = MRC(phi = 'fourier',use_cvx=True, loss = 'log').fit(X_train, y_train)\ndf_MRC = defDataFrame(model = clf_MRC,x_test = X_test, y_test = y_test)\nMRC_values = pd.DataFrame(df_MRC.Category.value_counts()).rename(columns = {'Category':type(clf_MRC).__name__})\nMRC_values['Freq_MRC'] = MRC_values['MRC']/sum(MRC_values['MRC'])*100\n\nclf_CMRC = CMRC(phi = 'threshold',use_cvx=True, loss = 'log').fit(X_train, y_train)\ndf_CMRC = defDataFrame(model = clf_CMRC,x_test = X_test, y_test = y_test)\nCMRC_values = pd.DataFrame(df_CMRC.Category.value_counts()).rename(columns = {'Category':type(clf_CMRC).__name__})\nCMRC_values['Freq_CMRC'] = CMRC_values['CMRC']/sum(CMRC_values['CMRC'])*100\n\nclf_SVC = SVC(probability = True).fit(X_train, y_train)\ndf_SVC = defDataFrame(model = clf_SVC,x_test = X_test, y_test = y_test)\nSVC_values = pd.DataFrame(df_SVC.Category.value_counts()).rename(columns = {'Category':type(clf_SVC).__name__})\nSVC_values['Freq_SVC'] = SVC_values['SVC']/sum(SVC_values['SVC'])*100\n\nclf_LR = LogisticRegression().fit(X_train, y_train)\ndf_LR = defDataFrame(model = clf_LR,x_test = X_test, y_test = y_test)\nLR_values = pd.DataFrame(df_LR.Category.value_counts()).rename(columns = {'Category':type(clf_LR).__name__})\nLR_values['Freq_LR'] = LR_values['LogisticRegression']/sum(LR_values['LogisticRegression'])*100\n\n\npd.concat([MRC_values, CMRC_values, SVC_values, LR_values],axis = 1).style.set_caption('Classification results by model').format(precision=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison of models:\nWe will compare now the histograms of the conditional probability for the \ntwo posible outcomes. Overlapping in the histograms means that the classification\nis erroneous. Condisering a cutoff of 0.5 pink cases below this point are \nfalse negatives (FN) and blue cases above the threhsold false positives (FP). \nIt is important to consider that in this classification problem the missclassification\nof a patient with fatal outcome (FN) is considered a much more serious error.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def scatterPlot(df, ax):\n    \"\"\"\n    Takes DF created with defDataFrame and creates a boxplot of different classification by mortal probability\n    \"\"\"\n    sns.swarmplot(ax = ax,y=\"Category\", x=\"Probabilities\", data=df,\n              size=4, palette=sns.color_palette(\"tab10\"), linewidth=0, dodge = False, alpha = 0.6,\n              order=[\"True Negative\", 'False Negative', \"True Positive\", 'False Positive'])\n    sns.boxplot(ax = ax,x=\"Probabilities\", y=\"Category\",\n           color=\"White\", \n            data=df,\n            order=[\"True Negative\", 'False Negative', \"True Positive\", 'False Positive'],\n            saturation = 15)\n    ax.set_xlabel('Probability of mortality')\n    ax.set_ylabel('')\ndef plotHisto(df,ax, threshold = 0.5, normalize = True):\n    \"\"\"\n    Takes DF created with defDataFrame and plots histograms based on the probability of mortality by real Status at a selected @threshold.\n    \"\"\"\n    if normalize:\n        norm_params = {'stat':\"density\", 'common_norm':False}\n    else:\n        norm_params = {}\n    sns.histplot(ax = ax,data=df[df[\"Real\"] ==1], x=\"Probabilities\", color=\"deeppink\", \n                 label=\"Deceased\",bins = 15, binrange=[0,1], alpha = 0.6, element = 'step', **norm_params)  \n    sns.histplot(ax = ax,data=df[df[\"Real\"] ==0], x=\"Probabilities\", color=\"dodgerblue\", \n                 label=\"Survived\",bins = 15, binrange=[0,1],alpha = 0.4,  element = 'step',**norm_params)\n    ax.axvline(threshold,0,1, linestyle = (0, (1, 10)), linewidth = 0.7, color = 'black')\n\n#visualize results   \nfig, ax = plt.subplots(nrows = 2,ncols = 2, sharex = 'all', \n                       sharey = 'all', gridspec_kw={'wspace':0.1,'hspace':0.35}) \nplotHisto(df_CMRC, ax = ax[0,0], normalize = False)\nax[0,0].set_title('CMRC')\nplotHisto(df_MRC, ax = ax[1,0], normalize = False)\nax[1,0].set_title('MRC')\nplotHisto(df_LR, ax = ax[0,1], normalize = False)\nax[0,1].set_title('LR')\nax[0,1].legend()\nplotHisto(df_SVC, ax = ax[1,1], normalize = False)\nax[1,1].set_title('SVC')\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see a clear different behaviour with the CMRC and MRC. MRC tends to estimate\nconditional probabilities in a more conservative way, rangin from 0.25 to 0.75. \nThis estimation is very sensible to cut-off changes. The CMRC model shows a \ndistribution where most of the cases are grouped around 0 and 1\nfor survive and decease respectively. This results are similar to the \nLogistic Regression's but with less overlapping. SVC is the model with the worst \nperformance of all having a lot of patients that survived with high decease \nprobabilities. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cm_cmrc = confusion_matrix(y_test, clf_CMRC.predict(X_test)) #CMRC\ncm_mrc = confusion_matrix(y_test, clf_MRC.predict(X_test)) #MRC\ncm_lr = confusion_matrix(y_test, clf_LR.predict(X_test)) #Logistic Regression\ncm_svc = confusion_matrix(y_test, clf_SVC.predict(X_test)) #C-Support Vector Machine\n\nfig, ax = plt.subplots(nrows = 2,ncols = 2, sharex = 'all', sharey = 'all', gridspec_kw={'wspace':0,'hspace':0.35}) \nConfusionMatrixDisplay(cm_cmrc, display_labels=['Survive', 'Decease']).plot(colorbar = False, ax = ax[0,0])\nax[0,0].set_title('CMRC')\nConfusionMatrixDisplay(cm_mrc, display_labels=['Survive', 'Decease']).plot(colorbar = False, ax = ax[1,0])\nax[1,0].set_title('MRC')\nConfusionMatrixDisplay(cm_lr, display_labels=['Survive', 'Decease']).plot(colorbar = False, ax = ax[0,1])\nax[0,1].set_title('LR')\nConfusionMatrixDisplay(cm_svc, display_labels=['Survive', 'Decease']).plot(colorbar = False, ax = ax[1,1])\nax[1,1].set_title('SVC')\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(classification_report(y_test, \n                                   clf_CMRC.predict(X_test), \n                                   target_names = ['Survive', 'Decease'], \n                                   output_dict=True)).style.set_caption('Classification report CMRC').format(precision=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(classification_report(y_test,\n                                   clf_MRC.predict(X_test),\n                                   target_names = ['Survive', 'Decease'],\n                                   output_dict=True)).style.set_caption('Classification report MRC').format(precision=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(classification_report(y_test,\n                                   clf_LR.predict(X_test),\n                                   target_names = ['Survive', 'Decease'],\n                                   output_dict=True)).style.set_caption('Classification report LR').format(precision=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(classification_report(y_test,\n                                   clf_SVC.predict(X_test),\n                                   target_names = ['Survive', 'Decease'],\n                                   output_dict=True)).style.set_caption('Classification report SVC').format(precision=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see in the classification reports and the confusion matrices the \noutperformance of CMRC. \n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Settind the cut-off point for binary classification:\nIn this section we will use beeswarm-boxplot to select the cut-off point\nto optimise the tradeoff between false positives and false negatives.\nThe beeswarm-boxplot is a great tool to determine the performance of the model\nin each of the cases of the confusion matrix. On an ideal scenario the errors\nare located near the cut-off point and the true guesses are located near the \n0 and 1 values. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(nrows = 2,ncols = 2, figsize = (10,12), sharex = 'all', \n                       sharey = 'all', gridspec_kw={'wspace':0.1,'hspace':0.20}) \nscatterPlot(df_CMRC, ax[0,0])\nax[0,0].set_title('CMRC')\nscatterPlot(df_MRC, ax[1,0])\nax[1,0].set_title('MRC')\nscatterPlot(df_LR, ax[0,1])\nax[0,1].set_title('LR')\nscatterPlot(df_SVC, ax[1,1])\nax[1,1].set_title('SVC')\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see in the CMRC that the correct cases have a very good\nconditional probability estimation with around 75% of the cases very close to \nthe extreme values. The most problematic cases are those with a low mortality \nprobability estimation that had a fatal outcome (FN). In the CMRC\nmodel adjusting the threshold to 0.35 reduces the false negatives by 25% \nadding just some cases to the FP. In the MRC model adjusting the cutoff to \n0.4 reduces half of the false negatives by trading of 25% of the TP.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "threshold = 0.35\ndf_CMRC = defDataFrame(model = clf_CMRC,x_test = X_test, y_test = y_test, threshold = threshold)\nthreshold = 0.4\ndf_MRC = defDataFrame(model = clf_MRC,x_test = X_test, y_test = y_test,  threshold = threshold)\npd.DataFrame(classification_report(df_CMRC.Real, \n                                   df_CMRC.Prediction, \n                                   target_names = ['Survive', 'Decease'], \n                                   output_dict=True)).style.set_caption('Classification report CMRC \\n adjusted threshold').format(precision=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(classification_report(df_MRC.Real,\n                                   df_MRC.Prediction,\n                                   target_names = ['Survive', 'Decease'],\n                                   output_dict=True)).style.set_caption('Classification report MRC \\n adjusted threshold').format(precision=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results:\nComparing the outputs of this example we can determine that MRCs work \nsignificantly well for estimating the outcome of COVID-19 patients at hospital triage. \n\nFurthermore, the CMRC model with threhsold feature mapping has shown a great \nperformance both for classifying and for estimating conditional probabilities\nFinally we have seen how to select the cut-off values based on data visualization\nwith beeswarm-boxplots to increase the recall in the desired class.  \n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}