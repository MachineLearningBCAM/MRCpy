
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples\further_examples\plot_comparison.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_further_examples_plot_comparison.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_further_examples_plot_comparison.py:


.. _ex_comp:

Example: Comparison to other methods
========================================
We are training and testing both MRC and CMRC methods with
a variety of different settings and comparing their performance both
error-wise and time-wise to other usual classification methods.

We will see that the performance of the MRC methods with the appropiate
settings is similar to the one of other methods like SVC (SVM Classification)
or MLPClassifier (neural network).
Furthermore, with non-determinitic approach and loss 0-1,
MRC method provides a theoretical upper and lower bound for the error
that can be an useful non-biased indicator of the performance of the
algorithm on a given dataset.
It also can be used to perform hyperparameter tuning in a much faster way than
cross-validation, you can check an example about that :ref:`here<grid>`.

We show the numerical results in three tables; the two firsts ones for all
the MRC and CMRC variants and the next one for all the comparison methods
in the deterministic and non-deterministic case respectively.
In these firsts tables the columns named 'upper' and 'lower' show the
upper and lower bound provided by the MRC method.
Note that in the case where loss = `0-1` these are upper and
lower bounds of the classification error while, in the case of `loss=log`
these bounds correspond to the log-likelihood.

Note that we set the parameter use_cvx=False. In the case of MRC classifiers
this means that we will use nesterov subgradient optimized approach to
perform the optimization. In the case of CMRC classifiers it will use the fast
Stochastic Gradient Descent (SGD) approach for linear and random fourier
feature mappings and nesterov subgradient approach for the rest of feature
mappings.

.. GENERATED FROM PYTHON SOURCE LINES 37-57

.. code-block:: default


    # Import needed modules
    import time

    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    from sklearn import preprocessing
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import KFold
    from sklearn.neural_network import MLPClassifier
    from sklearn.svm import SVC

    from MRCpy import CMRC, MRC
    from MRCpy.datasets import load_credit, load_haberman


    KFOLDS = 5
    kf = KFold(n_splits=KFOLDS)








.. GENERATED FROM PYTHON SOURCE LINES 58-66

MRC and CMRC methods
^^^^^^^^^^^^
We are training and testing both MRC and CMRC methods with
a variety of different settings; using 0-1 loss and logarithmic loss, using
all the default feature mappings available (Linear, Random Fourier, ReLU,
Threshold) and using both the non-deterministic and deterministic
approach which uses or not,
respectively probability estimates in the prediction stage.

.. GENERATED FROM PYTHON SOURCE LINES 66-142

.. code-block:: default



    def runMRC(X, Y):
        df_mrc = pd.DataFrame(np.zeros((8, 4)),
                              columns=['MRC', 'MRC time', 'CMRC', 'CMRC time'],
                              index=['loss 0-1, phi linear',
                                     'loss 0-1, phi fourier',
                                     'loss 0-1, phi relu',
                                     'loss 0-1, phi threshold',
                                     'loss log, phi linear',
                                     'loss log, phi fourier',
                                     'loss log, phi relu',
                                     'loss log, phi threshold'])

        df_mrc_nd = pd.DataFrame(np.zeros((4, 4)),
                                 columns=['MRC', 'MRC time', 'upper', 'lower'],
                                 index=['loss 0-1, phi linear',
                                        'loss 0-1, phi fourier',
                                        'loss 0-1, phi relu',
                                        'loss 0-1, phi threshold'])

        for train_index, test_index in kf.split(X):
            X_train, X_test = X[train_index], X[test_index]
            Y_train, Y_test = Y[train_index], Y[test_index]
            std_scale = preprocessing.StandardScaler().fit(X_train, Y_train)
            X_train = std_scale.transform(X_train)
            X_test = std_scale.transform(X_test)

            for loss in ['0-1', 'log']:
                for phi in ['linear', 'fourier', 'relu', 'threshold']:
                    row_name = 'loss ' + loss + ', phi ' + phi

                    # Deterministic case
                    startTime = time.time()
                    clf = MRC(loss=loss, phi=phi, random_state=0, sigma='scale',
                              deterministic=True, use_cvx=False
                              ).fit(X_train, Y_train)
                    Y_pred = clf.predict(X_test)
                    error = np.average(Y_pred != Y_test)
                    totalTime = time.time() - startTime

                    df_mrc['MRC time'][row_name] += totalTime
                    df_mrc['MRC'][row_name] += error

                    startTime = time.time()
                    clf = CMRC(loss=loss, phi=phi, random_state=0, sigma='scale',
                               deterministic=True, use_cvx=False,
                               ).fit(X_train, Y_train)
                    Y_pred = clf.predict(X_test)
                    error = np.average(Y_pred != Y_test)
                    totalTime = time.time() - startTime

                    df_mrc['CMRC time'][row_name] += totalTime
                    df_mrc['CMRC'][row_name] += error

                    if loss == '0-1':
                        # Non-deterministic case (with upper-lower bounds)
                        startTime = time.time()
                        clf = MRC(loss=loss, phi=phi, random_state=0,
                                  sigma='scale',
                                  deterministic=False, use_cvx=False,
                                  ).fit(X_train, Y_train)
                        Y_pred = clf.predict(X_test)
                        error = np.average(Y_pred != Y_test)
                        totalTime = time.time() - startTime

                        df_mrc_nd['MRC time'][row_name] += totalTime
                        df_mrc_nd['MRC'][row_name] += error
                        df_mrc_nd['upper'][row_name] += clf.get_upper_bound()
                        df_mrc_nd['lower'][row_name] += clf.get_lower_bound()

        df_mrc = df_mrc.divide(KFOLDS)
        df_mrc_nd = df_mrc_nd.divide(KFOLDS)
        return df_mrc, df_mrc_nd









.. GENERATED FROM PYTHON SOURCE LINES 143-146

Note that the non deterministic linear case is expected to perform poorly
for datasets with small initial dimensions
like the ones in the example.

.. GENERATED FROM PYTHON SOURCE LINES 146-153

.. code-block:: default


    # Credit dataset
    X, Y = load_credit()
    df_mrc_credit, df_mrc_nd_credit = runMRC(X, Y)
    df_mrc_credit.style.set_caption('Credit Dataset: Deterministic \
                                    MRC and CMRC error and runtime')






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style type="text/css">
    </style>
    <table id="T_4477c_">
      <caption>Credit Dataset: Deterministic                                 MRC and CMRC error and runtime</caption>
      <thead>
        <tr>
          <th class="blank level0" >&nbsp;</th>
          <th class="col_heading level0 col0" >MRC</th>
          <th class="col_heading level0 col1" >MRC time</th>
          <th class="col_heading level0 col2" >CMRC</th>
          <th class="col_heading level0 col3" >CMRC time</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th id="T_4477c_level0_row0" class="row_heading level0 row0" >loss 0-1, phi linear</th>
          <td id="T_4477c_row0_col0" class="data row0 col0" >0.146377</td>
          <td id="T_4477c_row0_col1" class="data row0 col1" >0.632696</td>
          <td id="T_4477c_row0_col2" class="data row0 col2" >0.169565</td>
          <td id="T_4477c_row0_col3" class="data row0 col3" >0.411301</td>
        </tr>
        <tr>
          <th id="T_4477c_level0_row1" class="row_heading level0 row1" >loss 0-1, phi fourier</th>
          <td id="T_4477c_row1_col0" class="data row1 col0" >0.155072</td>
          <td id="T_4477c_row1_col1" class="data row1 col1" >0.761953</td>
          <td id="T_4477c_row1_col2" class="data row1 col2" >0.179710</td>
          <td id="T_4477c_row1_col3" class="data row1 col3" >0.606784</td>
        </tr>
        <tr>
          <th id="T_4477c_level0_row2" class="row_heading level0 row2" >loss 0-1, phi relu</th>
          <td id="T_4477c_row2_col0" class="data row2 col0" >0.146377</td>
          <td id="T_4477c_row2_col1" class="data row2 col1" >1.016141</td>
          <td id="T_4477c_row2_col2" class="data row2 col2" >0.160870</td>
          <td id="T_4477c_row2_col3" class="data row2 col3" >7.212715</td>
        </tr>
        <tr>
          <th id="T_4477c_level0_row3" class="row_heading level0 row3" >loss 0-1, phi threshold</th>
          <td id="T_4477c_row3_col0" class="data row3 col0" >0.147826</td>
          <td id="T_4477c_row3_col1" class="data row3 col1" >1.066548</td>
          <td id="T_4477c_row3_col2" class="data row3 col2" >0.176812</td>
          <td id="T_4477c_row3_col3" class="data row3 col3" >7.587780</td>
        </tr>
        <tr>
          <th id="T_4477c_level0_row4" class="row_heading level0 row4" >loss log, phi linear</th>
          <td id="T_4477c_row4_col0" class="data row4 col0" >0.146377</td>
          <td id="T_4477c_row4_col1" class="data row4 col1" >1.434835</td>
          <td id="T_4477c_row4_col2" class="data row4 col2" >0.159420</td>
          <td id="T_4477c_row4_col3" class="data row4 col3" >0.540798</td>
        </tr>
        <tr>
          <th id="T_4477c_level0_row5" class="row_heading level0 row5" >loss log, phi fourier</th>
          <td id="T_4477c_row5_col0" class="data row5 col0" >0.157971</td>
          <td id="T_4477c_row5_col1" class="data row5 col1" >2.952136</td>
          <td id="T_4477c_row5_col2" class="data row5 col2" >0.184058</td>
          <td id="T_4477c_row5_col3" class="data row5 col3" >0.762561</td>
        </tr>
        <tr>
          <th id="T_4477c_level0_row6" class="row_heading level0 row6" >loss log, phi relu</th>
          <td id="T_4477c_row6_col0" class="data row6 col0" >0.450725</td>
          <td id="T_4477c_row6_col1" class="data row6 col1" >3.201041</td>
          <td id="T_4477c_row6_col2" class="data row6 col2" >0.268116</td>
          <td id="T_4477c_row6_col3" class="data row6 col3" >5.586820</td>
        </tr>
        <tr>
          <th id="T_4477c_level0_row7" class="row_heading level0 row7" >loss log, phi threshold</th>
          <td id="T_4477c_row7_col0" class="data row7 col0" >0.146377</td>
          <td id="T_4477c_row7_col1" class="data row7 col1" >22.081760</td>
          <td id="T_4477c_row7_col2" class="data row7 col2" >0.159420</td>
          <td id="T_4477c_row7_col3" class="data row7 col3" >16.742233</td>
        </tr>
      </tbody>
    </table>

    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 154-159

.. code-block:: default


    df_mrc_nd_credit.style.set_caption('Credit Dataset: Non-Deterministic \
                                       MRC error and runtime\nwith Upper and\
                                           Lower bounds')






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style type="text/css">
    </style>
    <table id="T_0e01f_">
      <caption>Credit Dataset: Non-Deterministic                                    MRC error and runtime
    with Upper and                                       Lower bounds</caption>
      <thead>
        <tr>
          <th class="blank level0" >&nbsp;</th>
          <th class="col_heading level0 col0" >MRC</th>
          <th class="col_heading level0 col1" >MRC time</th>
          <th class="col_heading level0 col2" >upper</th>
          <th class="col_heading level0 col3" >lower</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th id="T_0e01f_level0_row0" class="row_heading level0 row0" >loss 0-1, phi linear</th>
          <td id="T_0e01f_row0_col0" class="data row0 col0" >0.146377</td>
          <td id="T_0e01f_row0_col1" class="data row0 col1" >0.639091</td>
          <td id="T_0e01f_row0_col2" class="data row0 col2" >0.150177</td>
          <td id="T_0e01f_row0_col3" class="data row0 col3" >0.136525</td>
        </tr>
        <tr>
          <th id="T_0e01f_level0_row1" class="row_heading level0 row1" >loss 0-1, phi fourier</th>
          <td id="T_0e01f_row1_col0" class="data row1 col0" >0.202899</td>
          <td id="T_0e01f_row1_col1" class="data row1 col1" >0.765952</td>
          <td id="T_0e01f_row1_col2" class="data row1 col2" >0.185676</td>
          <td id="T_0e01f_row1_col3" class="data row1 col3" >0.137438</td>
        </tr>
        <tr>
          <th id="T_0e01f_level0_row2" class="row_heading level0 row2" >loss 0-1, phi relu</th>
          <td id="T_0e01f_row2_col0" class="data row2 col0" >0.182609</td>
          <td id="T_0e01f_row2_col1" class="data row2 col1" >1.029247</td>
          <td id="T_0e01f_row2_col2" class="data row2 col2" >0.188265</td>
          <td id="T_0e01f_row2_col3" class="data row2 col3" >0.098250</td>
        </tr>
        <tr>
          <th id="T_0e01f_level0_row3" class="row_heading level0 row3" >loss 0-1, phi threshold</th>
          <td id="T_0e01f_row3_col0" class="data row3 col0" >0.150725</td>
          <td id="T_0e01f_row3_col1" class="data row3 col1" >1.071576</td>
          <td id="T_0e01f_row3_col2" class="data row3 col2" >0.163806</td>
          <td id="T_0e01f_row3_col3" class="data row3 col3" >0.119446</td>
        </tr>
      </tbody>
    </table>

    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 160-167

.. code-block:: default


    # Haberman Dataset
    X, Y = load_haberman()
    df_mrc_haberman, df_mrc_nd_haberman = runMRC(X, Y)
    df_mrc_haberman.style.set_caption('Haberman Dataset: Deterministic \
                                      MRC and CMRC error and runtime')






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style type="text/css">
    </style>
    <table id="T_80dcf_">
      <caption>Haberman Dataset: Deterministic                                   MRC and CMRC error and runtime</caption>
      <thead>
        <tr>
          <th class="blank level0" >&nbsp;</th>
          <th class="col_heading level0 col0" >MRC</th>
          <th class="col_heading level0 col1" >MRC time</th>
          <th class="col_heading level0 col2" >CMRC</th>
          <th class="col_heading level0 col3" >CMRC time</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th id="T_80dcf_level0_row0" class="row_heading level0 row0" >loss 0-1, phi linear</th>
          <td id="T_80dcf_row0_col0" class="data row0 col0" >0.268324</td>
          <td id="T_80dcf_row0_col1" class="data row0 col1" >0.438615</td>
          <td id="T_80dcf_row0_col2" class="data row0 col2" >0.268059</td>
          <td id="T_80dcf_row0_col3" class="data row0 col3" >0.393553</td>
        </tr>
        <tr>
          <th id="T_80dcf_level0_row1" class="row_heading level0 row1" >loss 0-1, phi fourier</th>
          <td id="T_80dcf_row1_col0" class="data row1 col0" >0.261766</td>
          <td id="T_80dcf_row1_col1" class="data row1 col1" >0.592838</td>
          <td id="T_80dcf_row1_col2" class="data row1 col2" >0.300529</td>
          <td id="T_80dcf_row1_col3" class="data row1 col3" >0.578653</td>
        </tr>
        <tr>
          <th id="T_80dcf_level0_row2" class="row_heading level0 row2" >loss 0-1, phi relu</th>
          <td id="T_80dcf_row2_col0" class="data row2 col0" >0.274722</td>
          <td id="T_80dcf_row2_col1" class="data row2 col1" >0.632110</td>
          <td id="T_80dcf_row2_col2" class="data row2 col2" >0.284400</td>
          <td id="T_80dcf_row2_col3" class="data row2 col3" >2.895857</td>
        </tr>
        <tr>
          <th id="T_80dcf_level0_row3" class="row_heading level0 row3" >loss 0-1, phi threshold</th>
          <td id="T_80dcf_row3_col0" class="data row3 col0" >0.294289</td>
          <td id="T_80dcf_row3_col1" class="data row3 col1" >0.464359</td>
          <td id="T_80dcf_row3_col2" class="data row3 col2" >0.277895</td>
          <td id="T_80dcf_row3_col3" class="data row3 col3" >2.089812</td>
        </tr>
        <tr>
          <th id="T_80dcf_level0_row4" class="row_heading level0 row4" >loss log, phi linear</th>
          <td id="T_80dcf_row4_col0" class="data row4 col0" >0.268324</td>
          <td id="T_80dcf_row4_col1" class="data row4 col1" >0.948077</td>
          <td id="T_80dcf_row4_col2" class="data row4 col2" >0.261766</td>
          <td id="T_80dcf_row4_col3" class="data row4 col3" >0.548983</td>
        </tr>
        <tr>
          <th id="T_80dcf_level0_row5" class="row_heading level0 row5" >loss log, phi fourier</th>
          <td id="T_80dcf_row5_col0" class="data row5 col0" >0.265045</td>
          <td id="T_80dcf_row5_col1" class="data row5 col1" >1.462354</td>
          <td id="T_80dcf_row5_col2" class="data row5 col2" >0.287573</td>
          <td id="T_80dcf_row5_col3" class="data row5 col3" >0.754782</td>
        </tr>
        <tr>
          <th id="T_80dcf_level0_row6" class="row_heading level0 row6" >loss log, phi relu</th>
          <td id="T_80dcf_row6_col0" class="data row6 col0" >0.274722</td>
          <td id="T_80dcf_row6_col1" class="data row6 col1" >1.465286</td>
          <td id="T_80dcf_row6_col2" class="data row6 col2" >0.313591</td>
          <td id="T_80dcf_row6_col3" class="data row6 col3" >2.216316</td>
        </tr>
        <tr>
          <th id="T_80dcf_level0_row7" class="row_heading level0 row7" >loss log, phi threshold</th>
          <td id="T_80dcf_row7_col0" class="data row7 col0" >0.284453</td>
          <td id="T_80dcf_row7_col1" class="data row7 col1" >1.237304</td>
          <td id="T_80dcf_row7_col2" class="data row7 col2" >0.274511</td>
          <td id="T_80dcf_row7_col3" class="data row7 col3" >0.507653</td>
        </tr>
      </tbody>
    </table>

    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 168-173

.. code-block:: default


    df_mrc_nd_haberman.style.set_caption('Haberman Dataset: Non-Deterministic MRC \
                                         error and runtime\nwith Upper and \
                                             Lower bounds')






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style type="text/css">
    </style>
    <table id="T_54af9_">
      <caption>Haberman Dataset: Non-Deterministic MRC                                      error and runtime
    with Upper and                                          Lower bounds</caption>
      <thead>
        <tr>
          <th class="blank level0" >&nbsp;</th>
          <th class="col_heading level0 col0" >MRC</th>
          <th class="col_heading level0 col1" >MRC time</th>
          <th class="col_heading level0 col2" >upper</th>
          <th class="col_heading level0 col3" >lower</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th id="T_54af9_level0_row0" class="row_heading level0 row0" >loss 0-1, phi linear</th>
          <td id="T_54af9_row0_col0" class="data row0 col0" >0.281227</td>
          <td id="T_54af9_row0_col1" class="data row0 col1" >0.438221</td>
          <td id="T_54af9_row0_col2" class="data row0 col2" >0.271850</td>
          <td id="T_54af9_row0_col3" class="data row0 col3" >0.254457</td>
        </tr>
        <tr>
          <th id="T_54af9_level0_row1" class="row_heading level0 row1" >loss 0-1, phi fourier</th>
          <td id="T_54af9_row1_col0" class="data row1 col0" >0.294183</td>
          <td id="T_54af9_row1_col1" class="data row1 col1" >0.596006</td>
          <td id="T_54af9_row1_col2" class="data row1 col2" >0.262187</td>
          <td id="T_54af9_row1_col3" class="data row1 col3" >0.235597</td>
        </tr>
        <tr>
          <th id="T_54af9_level0_row2" class="row_heading level0 row2" >loss 0-1, phi relu</th>
          <td id="T_54af9_row2_col0" class="data row2 col0" >0.274617</td>
          <td id="T_54af9_row2_col1" class="data row2 col1" >0.639290</td>
          <td id="T_54af9_row2_col2" class="data row2 col2" >0.264509</td>
          <td id="T_54af9_row2_col3" class="data row2 col3" >0.219872</td>
        </tr>
        <tr>
          <th id="T_54af9_level0_row3" class="row_heading level0 row3" >loss 0-1, phi threshold</th>
          <td id="T_54af9_row3_col0" class="data row3 col0" >0.261555</td>
          <td id="T_54af9_row3_col1" class="data row3 col1" >0.471539</td>
          <td id="T_54af9_row3_col2" class="data row3 col2" >0.258158</td>
          <td id="T_54af9_row3_col3" class="data row3 col3" >0.235543</td>
        </tr>
      </tbody>
    </table>

    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 174-189

SVM, Neural Networks: MLP Classifier, Random Forest Classifier
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Now, let's try other usual supervised classification algorithms and compare
the results.
For comparison purposes. We try the same experiment using the Support Vector
Machine method using C-Support Vector Classification implemented in the
:ref:`SVC<https://scikit-learn.org/stable/modules/
generated/sklearn.svm.SVC.html>`
function, the Neural Network
method :ref:`Multi-layer Perceptron classifier<https://scikit-learn.org/
stable/modules/generated/sklearn.neural_network.MLPClassifier.html>`
and a :ref:`Random Forest
Classifier<https://scikit-learn.org/stable/modules/generated/
sklearn.ensemble.RandomForestClassifier.html>`.
All of them from the library `scikit-learn`.

.. GENERATED FROM PYTHON SOURCE LINES 189-244

.. code-block:: default



    def runComparisonMethods(X, Y):
        df = pd.DataFrame(columns=['Method', 'Error', 'Time'])

        error_svm = 0
        totalTime_svm = 0
        error_mlp = 0
        totalTime_mlp = 0
        error_rf = 0
        totalTime_rf = 0

        for train_index, test_index in kf.split(X):
            X_train, X_test = X[train_index], X[test_index]
            Y_train, Y_test = Y[train_index], Y[test_index]
            std_scale = preprocessing.StandardScaler().fit(X_train, Y_train)
            X_train = std_scale.transform(X_train)
            X_test = std_scale.transform(X_test)

            startTime = time.time()
            clf = SVC(random_state=0).fit(X_train, Y_train)
            Y_pred = clf.predict(X_test)
            error_svm += np.average(Y_pred != Y_test)
            totalTime_svm += time.time() - startTime

            startTime = time.time()
            clf = MLPClassifier(random_state=0).fit(X_train, Y_train)
            Y_pred = clf.predict(X_test)
            error_mlp += np.average(Y_pred != Y_test)
            totalTime_mlp += time.time() - startTime

            startTime = time.time()
            clf = clf = RandomForestClassifier(
                max_depth=2, random_state=0).fit(X_train, Y_train)
            Y_pred = clf.predict(X_test)
            error_rf += np.average(Y_pred != Y_test)
            totalTime_rf += time.time() - startTime

        error_svm /= KFOLDS
        totalTime_svm /= KFOLDS
        error_mlp /= KFOLDS
        totalTime_mlp /= KFOLDS
        error_rf /= KFOLDS
        totalTime_rf /= KFOLDS

        df = df.append({'Method': 'SVM', 'Error': error_svm,
                        'Time': totalTime_svm}, ignore_index=True)
        df = df.append({'Method': 'NN-MLP', 'Error': error_mlp,
                        'Time': totalTime_mlp}, ignore_index=True)
        df = df.append({'Method': 'Random Forest', 'Error': error_rf,
                        'Time': totalTime_rf}, ignore_index=True)
        df = df.set_index('Method')
        return df









.. GENERATED FROM PYTHON SOURCE LINES 245-252

.. code-block:: default


    # Credit Dataset
    X, Y = load_credit()
    df_credit = runComparisonMethods(X, Y)
    df_credit.style.set_caption('Credit Dataset: Different \
                                methods error and runtime')






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style type="text/css">
    </style>
    <table id="T_5bb13_">
      <caption>Credit Dataset: Different                             methods error and runtime</caption>
      <thead>
        <tr>
          <th class="blank level0" >&nbsp;</th>
          <th class="col_heading level0 col0" >Error</th>
          <th class="col_heading level0 col1" >Time</th>
        </tr>
        <tr>
          <th class="index_name level0" >Method</th>
          <th class="blank col0" >&nbsp;</th>
          <th class="blank col1" >&nbsp;</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th id="T_5bb13_level0_row0" class="row_heading level0 row0" >SVM</th>
          <td id="T_5bb13_row0_col0" class="data row0 col0" >0.166667</td>
          <td id="T_5bb13_row0_col1" class="data row0 col1" >0.011369</td>
        </tr>
        <tr>
          <th id="T_5bb13_level0_row1" class="row_heading level0 row1" >NN-MLP</th>
          <td id="T_5bb13_row1_col0" class="data row1 col0" >0.150725</td>
          <td id="T_5bb13_row1_col1" class="data row1 col1" >0.373402</td>
        </tr>
        <tr>
          <th id="T_5bb13_level0_row2" class="row_heading level0 row2" >Random Forest</th>
          <td id="T_5bb13_row2_col0" class="data row2 col0" >0.165217</td>
          <td id="T_5bb13_row2_col1" class="data row2 col1" >0.107313</td>
        </tr>
      </tbody>
    </table>

    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 253-260

.. code-block:: default


    # Haberman Dataset
    X, Y = load_haberman()
    df_haberman = runComparisonMethods(X, Y)
    df_haberman.style.set_caption('Haberman Dataset: Different \
                                  methods error and runtime')






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style type="text/css">
    </style>
    <table id="T_1cf8d_">
      <caption>Haberman Dataset: Different                               methods error and runtime</caption>
      <thead>
        <tr>
          <th class="blank level0" >&nbsp;</th>
          <th class="col_heading level0 col0" >Error</th>
          <th class="col_heading level0 col1" >Time</th>
        </tr>
        <tr>
          <th class="index_name level0" >Method</th>
          <th class="blank col0" >&nbsp;</th>
          <th class="blank col1" >&nbsp;</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th id="T_1cf8d_level0_row0" class="row_heading level0 row0" >SVM</th>
          <td id="T_1cf8d_row0_col0" class="data row0 col0" >0.258488</td>
          <td id="T_1cf8d_row0_col1" class="data row0 col1" >0.002992</td>
        </tr>
        <tr>
          <th id="T_1cf8d_level0_row1" class="row_heading level0 row1" >NN-MLP</th>
          <td id="T_1cf8d_row1_col0" class="data row1 col0" >0.284294</td>
          <td id="T_1cf8d_row1_col1" class="data row1 col1" >0.197672</td>
        </tr>
        <tr>
          <th id="T_1cf8d_level0_row2" class="row_heading level0 row2" >Random Forest</th>
          <td id="T_1cf8d_row2_col0" class="data row2 col0" >0.274828</td>
          <td id="T_1cf8d_row2_col1" class="data row2 col1" >0.098536</td>
        </tr>
      </tbody>
    </table>

    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 261-277

Comparison of MRCs to other methods
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
In the deterministic case we can see that the performance of MRC and CMRC
methods in the
appropiate settings is similar to usual methods such as SVM and
Neural Networks implemented by the MLPClassifier. Best performances for MRC
method are usually reached using loss = `0-1` and phi = `fourier` or
phi = `relu`. Even though these
settings make the execution time of MRC a little bit higher than others it
is still  similar to the time it would take to use the MLPClassifier.

Now we are plotting some figures for the **deterministic** case.

Note that
the options of MRC with loss = `0-1` use an optimized version of Nesterov
optimization algorithm, improving the runtime of these options.

.. GENERATED FROM PYTHON SOURCE LINES 277-320

.. code-block:: default



    # Graph plotting
    def major_formatter(x, pos):
        label = '' if x < 0 else '%0.2f' % x
        return label


    def major_formatter1(x, pos):
        label = '' if x < 0 or x > 0.16 else '%0.3f' % x
        return label


    def major_formatter2(x, pos):
        label = '' if x < 0 else '%0.2g' % x
        return label


    fig = plt.figure()
    ax = fig.add_axes([0, 0, 1, 1])
    labels = ['CMRC\n0-1\nlinear',
              'MRC\n0-1\nrelu',
              'MRC\n0-1\nthreshold',
              'MRC\nlog\nthreshold',
              'SVM', 'NN-MLP',
              'Random\nforest']

    errors = [df_mrc_credit['CMRC']['loss 0-1, phi linear'],
              df_mrc_credit['MRC']['loss 0-1, phi relu'],
              df_mrc_credit['MRC']['loss 0-1, phi threshold'],
              df_mrc_credit['MRC']['loss log, phi threshold'],
              df_credit['Error']['SVM'],
              df_credit['Error']['NN-MLP'],
              df_credit['Error']['Random Forest']]
    ax.bar([''] + labels, [0] + errors, color='lightskyblue')
    plt.title('Credit Dataset Errors')
    ax.tick_params(axis="y", direction="in", pad=-35)
    ax.tick_params(axis="x", direction="out", pad=-40)
    ax.yaxis.set_major_formatter(major_formatter1)
    margin = 0.05 * max(errors)
    ax.set_ylim([-margin * 3.5, max(errors) + margin])
    plt.show()




.. image-sg:: /auto_examples/further_examples/images/sphx_glr_plot_comparison_001.png
   :alt: Credit Dataset Errors
   :srcset: /auto_examples/further_examples/images/sphx_glr_plot_comparison_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 321-324

Above: MRCs errors for different parameter settings
compared to other techniques for the dataset Credit. The ordinate
axis represents the error (proportion of incorrectly predicted labels).

.. GENERATED FROM PYTHON SOURCE LINES 326-349

.. code-block:: default


    fig = plt.figure()
    ax = fig.add_axes([0, 0, 1, 1])

    labels = ['MRC\n0-1\nrelu',
              'MRC\n0-1\nthreshold',
              'SVM', 'NN-MLP',
              'Random\nforest']

    times = [df_mrc_credit['MRC time']['loss 0-1, phi relu'],
             df_mrc_credit['MRC time']['loss 0-1, phi threshold'],
             df_credit['Time']['SVM'],
             df_credit['Time']['NN-MLP'],
             df_credit['Time']['Random Forest']]
    ax.bar([''] + labels, [0] + times, color='lightskyblue')
    plt.title('Credit Dataset Runtime')
    ax.tick_params(axis="y", direction="in", pad=-30)
    ax.tick_params(axis="x", direction="out", pad=-40)
    ax.yaxis.set_major_formatter(major_formatter2)
    margin = 0.05 * max(times)
    ax.set_ylim([-margin * 3.5, max(times) + margin])
    plt.show()




.. image-sg:: /auto_examples/further_examples/images/sphx_glr_plot_comparison_002.png
   :alt: Credit Dataset Runtime
   :srcset: /auto_examples/further_examples/images/sphx_glr_plot_comparison_002.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 350-353

Above: MRCs runtime for different parameter settings
compared to other techniques for the dataset Credit. The ordinate
represents the runtime measured in seconds.

.. GENERATED FROM PYTHON SOURCE LINES 355-377

.. code-block:: default

    fig = plt.figure()
    ax = fig.add_axes([0, 0, 1, 1])
    labels = ['MRC\n0-1\nfourier',
              'CMRC\n0-1\nfourier',
              'SVM',
              'NN-MLP',
              'Random\nforest']

    errors = [df_mrc_haberman['MRC']['loss 0-1, phi fourier'],
              df_mrc_haberman['CMRC']['loss 0-1, phi fourier'],
              df_haberman['Error']['SVM'],
              df_haberman['Error']['NN-MLP'],
              df_haberman['Error']['Random Forest']]
    ax.bar([''] + labels, [0] + errors, color='lightskyblue')
    plt.title('Haberman Dataset Errors')
    ax.tick_params(axis="y", direction="in", pad=-30)
    ax.tick_params(axis="x", direction="out", pad=-40)
    ax.yaxis.set_major_formatter(major_formatter)
    margin = 0.05 * max(errors)
    ax.set_ylim([-margin * 3.5, max(errors) + margin])
    plt.show()




.. image-sg:: /auto_examples/further_examples/images/sphx_glr_plot_comparison_003.png
   :alt: Haberman Dataset Errors
   :srcset: /auto_examples/further_examples/images/sphx_glr_plot_comparison_003.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 378-381

Above: MRCs errors for different parameter settings
compared to other techniques for the dataset Haberman. The ordinate
axis represents the error (proportion of incorrectly predicted labels).

.. GENERATED FROM PYTHON SOURCE LINES 383-406

.. code-block:: default


    fig = plt.figure()
    ax = fig.add_axes([0, 0, 1, 1])

    labels = ['MRC\n0-1\nfourier',
              'MRC\n0-1\nrelu',
              'SVM', 'NN-MLP',
              'Random\nforest']

    times = [df_mrc_haberman['MRC time']['loss 0-1, phi fourier'],
             df_mrc_haberman['MRC time']['loss 0-1, phi relu'],
             df_haberman['Time']['SVM'],
             df_haberman['Time']['NN-MLP'],
             df_haberman['Time']['Random Forest']]
    ax.bar([''] + labels, [0] + times, color='lightskyblue')
    plt.title('Haberman Dataset Runtime')
    ax.tick_params(axis="y", direction="in", pad=-30)
    ax.tick_params(axis="x", direction="out", pad=-40)
    ax.yaxis.set_major_formatter(major_formatter2)
    margin = 0.05 * max(times)
    ax.set_ylim([-margin * 3.5, max(times) + margin])
    plt.show()




.. image-sg:: /auto_examples/further_examples/images/sphx_glr_plot_comparison_004.png
   :alt: Haberman Dataset Runtime
   :srcset: /auto_examples/further_examples/images/sphx_glr_plot_comparison_004.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 407-410

Above: MRCs runtime for different parameter settings
compared to other techniques for the dataset Haberman. The ordinate
represents the runtime measured in seconds.

.. GENERATED FROM PYTHON SOURCE LINES 412-425

Upper and Lower bounds provided by MRCs
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Furthermore, when using a non-deterministic approach and `loss = 0-1`, the
MRC method provides us with Upper and Lower theoretical bounds for the
error which can be of great use to make sure you are not overfitting your
model or for hyperparameter tuning. You can check our
:ref:`example on parameter tuning<grid>`.
In the logistic case these Upper and Lower values are the theoretical bounds
for the log-likelihood.

The only difference between the deterministic and  non-deterministic approach
is in the prediction stage so, as we can see, the runtime of both versions
is pretty similar.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 9 minutes  58.944 seconds)


.. _sphx_glr_download_auto_examples_further_examples_plot_comparison.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_comparison.py <plot_comparison.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_comparison.ipynb <plot_comparison.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
