{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Example: Use of MRC with different settings\n\nExample of using MRC with some of the common classification datasets with\ndifferent losses and feature mappings settings. We load the different datasets\nand use 10-Fold Cross-Validation to generate the partitions for train and test.\nWe separate 1 partition each time for testing and use the others for training.\nOn each iteration we calculate the classification error as well as the upper\nand lower bounds for the error. We also calculate the mean training time.\n\nNote that we set the parameter use_cvx=False. In the case of MRC classifiers\nthis means that we will use nesterov subgradient optimized approach to\nperform the optimization.\n\nYou can check a more elaborated example in `ex_comp`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom MRCpy import MRC\n# Import the datasets\nfrom MRCpy.datasets import *\n\n# Data sets\nloaders = [load_mammographic, load_haberman, load_indian_liver,\n           load_diabetes, load_credit]\ndataName = [\"mammographic\", \"haberman\", \"indian_liver\",\n            \"diabetes\", \"credit\"]\n\n\ndef runMRC(phi, loss):\n\n    results = pd.DataFrame()\n    # We fix the random seed to that the stratified kfold performed\n    # is the same through the different executions\n    random_seed = 0\n\n    # Iterate through each of the dataset and fit the MRC classfier.\n    for j, load in enumerate(loaders):\n\n        # Loading the dataset\n        X, Y = load()\n        r = len(np.unique(Y))\n        n, d = X.shape\n\n        clf = MRC(phi=phi, loss=loss, use_cvx=False)\n\n        # Generate the partitions of the stratified cross-validation\n        n_splits = 5\n        cv = StratifiedKFold(n_splits=n_splits, random_state=random_seed,\n                             shuffle=True)\n\n        cvError = list()\n        auxTime = 0\n        upper = 0\n        lower = 0\n\n        # Paired and stratified cross-validation\n        for train_index, test_index in cv.split(X, Y):\n\n            X_train, X_test = X[train_index], X[test_index]\n            y_train, y_test = Y[train_index], Y[test_index]\n\n            # Normalizing the data\n            std_scale = preprocessing.StandardScaler().fit(X_train, y_train)\n            X_train = std_scale.transform(X_train)\n            X_test = std_scale.transform(X_test)\n\n            # Save start time for computing training time\n            startTime = time.time()\n\n            # Train the model and save the upper and lower bounds\n            clf.fit(X_train, y_train)\n            upper += clf.get_upper_bound()\n            lower += clf.get_lower_bound()\n\n            # Save the training time\n            auxTime += time.time() - startTime\n\n            # Predict the class for test instances\n            y_pred = clf.predict(X_test)\n\n            # Calculate the error made by MRC classificator\n            cvError.append(np.average(y_pred != y_test))\n\n        res_mean = np.average(cvError)\n        res_std = np.std(cvError)\n\n        # Calculating the mean upper and lower bound and training time\n        upper = upper / n_splits\n        lower = lower / n_splits\n        auxTime = auxTime / n_splits\n\n        results = results.append({'dataset': dataName[j],\n                                  'n_samples': '%d' % n,\n                                  'n_attributes': '%d' % d,\n                                  'n_classes': '%d' % r,\n                                  'error': '%1.2g' % res_mean + \" +/- \" +\n                                  '%1.2g' % res_std,\n                                  'upper': '%1.2g' % upper,\n                                  'lower': '%1.2g' % lower,\n                                  'avg_train_time (s)': '%1.2g' % auxTime},\n                                 ignore_index=True)\n    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "r1 = runMRC(phi='fourier', loss='0-1')\nr1.style.set_caption('Using 0-1 loss and fourier feature mapping')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "r2 = runMRC(phi='fourier', loss='log')\nr2.style.set_caption('Using log loss and fourier feature mapping')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}